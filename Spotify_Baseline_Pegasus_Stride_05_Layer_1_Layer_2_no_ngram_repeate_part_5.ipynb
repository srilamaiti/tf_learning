{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/tf_learning/blob/main/Spotify_Baseline_Pegasus_Stride_05_Layer_1_Layer_2_no_ngram_repeate_part_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAbhr_vUe3Jf"
      },
      "outputs": [],
      "source": [
        "#import locale\n",
        "#locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SvA3_iefLHB"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q evaluate\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8bNzgOOfr6K",
        "outputId": "1bbd5ea7-10ca-43d5-87ae-c11446edbfd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.9/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from rouge) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q rouge_score\n",
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pMaVIY5ft4K"
      },
      "outputs": [],
      "source": [
        "#let's make longer output readable without horizontal scrolling\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX8Z1Y6IfzmT",
        "outputId": "ab1adcad-1674-4120-d4f7-092479800049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers version: 4.28.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "import os\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "os.environ['LANG'] = 'en_US.UTF-8'\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "#NLP related libraries\n",
        "import transformers\n",
        "print(f'transformers version: {transformers.__version__}')\n",
        "from transformers import logging as hf_logging\n",
        "#from transformers import BertTokenizer, TFBertModel\n",
        "hf_logging.set_verbosity_error()\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "from wordcloud import ImageColorGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('punkt')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "\n",
        "# Other required libraries\n",
        "#import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import copy\n",
        "import sys\n",
        "import warnings\n",
        "import io\n",
        "import math\n",
        "import torch\n",
        "import json\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# Data visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "from joblib import Parallel, delayed\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "\n",
        "\n",
        "# Tensorflow libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils.layer_utils import count_params\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.losses import mae\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.models import load_model\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "os.environ['LANG'] = 'en_US.UTF-8'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzF_iSmff2mQ",
        "outputId": "cf4777d1-fc96-41e0-9ecd-f7558989b5da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Required to read the data from Kaggle\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "os.environ['SPOTIFY_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Spotify\"\n",
        "\n",
        "print(os.environ.get('NotebookApp.iopub_data_rate_limit'))\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9SfTxfgf5SY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "os.environ['LANG'] = 'en_US.UTF-8'\n",
        "#os.chdir('/content/drive/MyDrive/Spotify')\n",
        "Original_path_file='/content/gdrive/MyDrive/Spotify/subsets/500/train.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory_path = \"/content/gdrive/MyDrive/Spotify/subsets/Model_Output/Pegasus_Extractive_Abstractive_L2/\"\n",
        "file_count = len(os.listdir(directory_path))\n",
        "\n",
        "print(f\"Number of files in directory: {file_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v53QZ1NZapp9",
        "outputId": "e3032a32-70eb-4f55-c08b-0786840a0458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in directory: 430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtUw_5E0f7zd",
        "outputId": "bb937c60-94d7-40fb-b083-78179f27ccf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "400\n"
          ]
        }
      ],
      "source": [
        "#df = pd.read_csv(io.BytesIO(Original_path_file))\n",
        " \n",
        "# df.columns\n",
        "\n",
        "with open(Original_path_file, 'rb') as f:\n",
        "    data = io.BytesIO(f.read())\n",
        "df_full = pd.read_csv(data)\n",
        "print(len(df_full))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqlx15hpEsH5",
        "outputId": "ee320cb1-7091-452b-d3dd-45770779eebb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-82df3b8e-560d-4384-b969-3f55f95a538f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>show_uri</th>\n",
              "      <th>show_name</th>\n",
              "      <th>show_description</th>\n",
              "      <th>publisher</th>\n",
              "      <th>language</th>\n",
              "      <th>rss_link</th>\n",
              "      <th>episode_uri</th>\n",
              "      <th>episode_name</th>\n",
              "      <th>episode_description</th>\n",
              "      <th>...</th>\n",
              "      <th>show_filename_prefix</th>\n",
              "      <th>episode_filename_prefix</th>\n",
              "      <th>word_count</th>\n",
              "      <th>cleaned_ep_desc</th>\n",
              "      <th>ID</th>\n",
              "      <th>Transcript</th>\n",
              "      <th>ep_desc_weirdchars</th>\n",
              "      <th>ep_desc_unidecoded</th>\n",
              "      <th>Transcript_weirdchars</th>\n",
              "      <th>Transcript_unidecoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>spotify:show:6R5ktHYRdfrm1ePuPcHeBe</td>\n",
              "      <td>Storytime with Jane</td>\n",
              "      <td>Listen to a new children’s storybook every day...</td>\n",
              "      <td>Jane</td>\n",
              "      <td>['en']</td>\n",
              "      <td>https://anchor.fm/s/126503d0/podcast/rss</td>\n",
              "      <td>spotify:episode:3jMe7zTRTwCm7q7VRqjDoX</td>\n",
              "      <td>The Gruffalo’s Child 📚</td>\n",
              "      <td>The little Gruffalo goes on an adventure throu...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_6R5ktHYRdfrm1ePuPcHeBe</td>\n",
              "      <td>3jMe7zTRTwCm7q7VRqjDoX</td>\n",
              "      <td>59</td>\n",
              "      <td>The little Gruffalo goes on an adventure throu...</td>\n",
              "      <td>3jMe7zTRTwCm7q7VRqjDoX</td>\n",
              "      <td>Welcome to story time. Today. We're going to b...</td>\n",
              "      <td>❄️🐁🌳🌲🐿🦔’📚</td>\n",
              "      <td>Welcome to story time. Today. We're going to b...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Welcome to story time. Today. We're going to b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>spotify:show:5MK1Xk5Yd9ATgQGg7D3P1m</td>\n",
              "      <td>The Sweet Sounds of ASMR</td>\n",
              "      <td>Welcome to the Sweet Sounds Of ASMR for those ...</td>\n",
              "      <td>Parris Youngblood</td>\n",
              "      <td>['en']</td>\n",
              "      <td>https://anchor.fm/s/89553a0/podcast/rss</td>\n",
              "      <td>spotify:episode:0bLEUbqPdsFOEPck4G6FGM</td>\n",
              "      <td>ASMR of Things</td>\n",
              "      <td>In this ASMR podcast I will be putting makeup ...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_5MK1Xk5Yd9ATgQGg7D3P1m</td>\n",
              "      <td>0bLEUbqPdsFOEPck4G6FGM</td>\n",
              "      <td>43</td>\n",
              "      <td>In this ASMR podcast I will be putting makeup ...</td>\n",
              "      <td>0bLEUbqPdsFOEPck4G6FGM</td>\n",
              "      <td>Guys, do you want to hear something amazing? A...</td>\n",
              "      <td></td>\n",
              "      <td>Guys, do you want to hear something amazing? A...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Guys, do you want to hear something amazing? A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>50</td>\n",
              "      <td>spotify:show:5mlrDhLAEhaaGszJezXvjF</td>\n",
              "      <td>Daily Christian Podcast</td>\n",
              "      <td>Listen in on how reading the bible every day i...</td>\n",
              "      <td>Paul Dziepak</td>\n",
              "      <td>['en-US']</td>\n",
              "      <td>https://anchor.fm/s/2ec6a88/podcast/rss</td>\n",
              "      <td>spotify:episode:32tNQkYcpgFOmWOrqpvPnp</td>\n",
              "      <td>Reflect on your journey.</td>\n",
              "      <td>As we start reading through the book of Deuter...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_5mlrDhLAEhaaGszJezXvjF</td>\n",
              "      <td>32tNQkYcpgFOmWOrqpvPnp</td>\n",
              "      <td>50</td>\n",
              "      <td>As we start reading through the book of Deuter...</td>\n",
              "      <td>32tNQkYcpgFOmWOrqpvPnp</td>\n",
              "      <td>It's pretty much the middle of the year right ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It's pretty much the middle of the year right ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It's pretty much the middle of the year right ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>52</td>\n",
              "      <td>spotify:show:4drmkFJHxGvu7X69hkjSUB</td>\n",
              "      <td>Affirmations by A.</td>\n",
              "      <td>Affirmations by A. delivers powerful affirmati...</td>\n",
              "      <td>Affirmations by A</td>\n",
              "      <td>['en']</td>\n",
              "      <td>https://anchor.fm/s/389bf18/podcast/rss</td>\n",
              "      <td>spotify:episode:1sYSra5nkgTMtnWuTq3yDA</td>\n",
              "      <td>34. Own Your Day</td>\n",
              "      <td>This episode was created to help you make a co...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_4drmkFJHxGvu7X69hkjSUB</td>\n",
              "      <td>1sYSra5nkgTMtnWuTq3yDA</td>\n",
              "      <td>85</td>\n",
              "      <td>This episode was created to help you make a co...</td>\n",
              "      <td>1sYSra5nkgTMtnWuTq3yDA</td>\n",
              "      <td>Hey guys, so if you haven't heard about anchor...</td>\n",
              "      <td>·</td>\n",
              "      <td>Hey guys, so if you haven't heard about anchor...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hey guys, so if you haven't heard about anchor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>53</td>\n",
              "      <td>spotify:show:1Ymux9Rk8sPpz7gtKdgeLR</td>\n",
              "      <td>Guru - The Alchemist</td>\n",
              "      <td>India market view Nifty Bank nifty Levels</td>\n",
              "      <td>MyBillion on the web</td>\n",
              "      <td>['en']</td>\n",
              "      <td>https://anchor.fm/s/c011628/podcast/rss</td>\n",
              "      <td>spotify:episode:7AUiSbcR4Rp8RgJvAS0I8i</td>\n",
              "      <td>India market view by Guru - The alchemist 27th...</td>\n",
              "      <td>Nifty, bank nifty views, levels. Last pod cast...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_1Ymux9Rk8sPpz7gtKdgeLR</td>\n",
              "      <td>7AUiSbcR4Rp8RgJvAS0I8i</td>\n",
              "      <td>42</td>\n",
              "      <td>Nifty, bank nifty views, levels. Last pod cast...</td>\n",
              "      <td>7AUiSbcR4Rp8RgJvAS0I8i</td>\n",
              "      <td>Good morning friends. Welcome to report cause ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Good morning friends. Welcome to report cause ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Good morning friends. Welcome to report cause ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>60</td>\n",
              "      <td>spotify:show:0NpVLVfKd8mtKlBjmEj0vu</td>\n",
              "      <td>Stoic Meditations</td>\n",
              "      <td>Occasional reflections on the wisdom of Ancien...</td>\n",
              "      <td>Massimo Pigliucci</td>\n",
              "      <td>['en-US']</td>\n",
              "      <td>https://anchor.fm/s/1D80328/podcast/rss</td>\n",
              "      <td>spotify:episode:7Iq72L6XjQBFDZncbuO70i</td>\n",
              "      <td>Be careful with the company you keep</td>\n",
              "      <td>We should live with the quietest and easiest-t...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_0NpVLVfKd8mtKlBjmEj0vu</td>\n",
              "      <td>7Iq72L6XjQBFDZncbuO70i</td>\n",
              "      <td>36</td>\n",
              "      <td>We should live with the quietest and easiest-t...</td>\n",
              "      <td>7Iq72L6XjQBFDZncbuO70i</td>\n",
              "      <td>Welcome to the stoic meditations podcast where...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Welcome to the stoic meditations podcast where...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Welcome to the stoic meditations podcast where...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>72</td>\n",
              "      <td>spotify:show:3c3gzFgW09IjpruwOHoEaC</td>\n",
              "      <td>Steven's weekly podcast</td>\n",
              "      <td>Steven is a thought leader on the transformati...</td>\n",
              "      <td>Steven Van Belleghem</td>\n",
              "      <td>['nl-BE']</td>\n",
              "      <td>https://anchor.fm/s/3693b44/podcast/rss</td>\n",
              "      <td>spotify:episode:5ptpXqYYvSw3TItmyzDZB6</td>\n",
              "      <td>Steven's week 134: News about Facebook, stream...</td>\n",
              "      <td>Hey guys, welcome to a new episode! Facebook a...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_3c3gzFgW09IjpruwOHoEaC</td>\n",
              "      <td>5ptpXqYYvSw3TItmyzDZB6</td>\n",
              "      <td>38</td>\n",
              "      <td>Hey guys, welcome to a new episode! Facebook a...</td>\n",
              "      <td>5ptpXqYYvSw3TItmyzDZB6</td>\n",
              "      <td>Hey guys. Thanks for watching this new episode...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hey guys. Thanks for watching this new episode...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hey guys. Thanks for watching this new episode...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>79</td>\n",
              "      <td>spotify:show:6tIYhCPdRlFvNrJcDG9uw4</td>\n",
              "      <td>Bunga Mega</td>\n",
              "      <td>Bunga Mega- known as coach, counselor &amp; book a...</td>\n",
              "      <td>Bunga Mega</td>\n",
              "      <td>['en']</td>\n",
              "      <td>https://anchor.fm/s/8fb7eb4/podcast/rss</td>\n",
              "      <td>spotify:episode:3ZXWygw71BQZxOw5Zpy49o</td>\n",
              "      <td>Marriage &amp; Family : Mau Mewariskan Pernikahan ...</td>\n",
              "      <td>Kita adalah produk nilai dan budaya pernikahan...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_6tIYhCPdRlFvNrJcDG9uw4</td>\n",
              "      <td>3ZXWygw71BQZxOw5Zpy49o</td>\n",
              "      <td>46</td>\n",
              "      <td>Kita adalah produk nilai dan budaya pernikahan...</td>\n",
              "      <td>3ZXWygw71BQZxOw5Zpy49o</td>\n",
              "      <td>Hello, Kappa slow mode that only podcast Mega ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hello, Kappa slow mode that only podcast Mega ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hello, Kappa slow mode that only podcast Mega ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>138</td>\n",
              "      <td>spotify:show:5mlrDhLAEhaaGszJezXvjF</td>\n",
              "      <td>Daily Christian Podcast</td>\n",
              "      <td>Listen in on how reading the bible every day i...</td>\n",
              "      <td>Paul Dziepak</td>\n",
              "      <td>['en-US']</td>\n",
              "      <td>https://anchor.fm/s/2ec6a88/podcast/rss</td>\n",
              "      <td>spotify:episode:5njlEFALN6ASvhJ0h3tD6g</td>\n",
              "      <td>I see you in your distress...</td>\n",
              "      <td>The Lord sees his people while they were in di...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_5mlrDhLAEhaaGszJezXvjF</td>\n",
              "      <td>5njlEFALN6ASvhJ0h3tD6g</td>\n",
              "      <td>27</td>\n",
              "      <td>The Lord sees his people while they were in di...</td>\n",
              "      <td>5njlEFALN6ASvhJ0h3tD6g</td>\n",
              "      <td>Well, Happy Father's day. It's all you awesome...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Well, Happy Father's day. It's all you awesome...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Well, Happy Father's day. It's all you awesome...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>150</td>\n",
              "      <td>spotify:show:3aFDEGgGGeXX28Sn9ZkAh1</td>\n",
              "      <td>TENI PODCAST</td>\n",
              "      <td>There are all sorts of ways to kill a perfectl...</td>\n",
              "      <td>leoshealth podcast</td>\n",
              "      <td>['en']</td>\n",
              "      <td>https://anchor.fm/s/11af33ac/podcast/rss</td>\n",
              "      <td>spotify:episode:0EpgpJCLJmNneFCJOgbH53</td>\n",
              "      <td>EP 00001 Car maintenance and servicing basics</td>\n",
              "      <td>If you were about to get on a plane and fly ac...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_3aFDEGgGGeXX28Sn9ZkAh1</td>\n",
              "      <td>0EpgpJCLJmNneFCJOgbH53</td>\n",
              "      <td>70</td>\n",
              "      <td>If you were about to get on a plane and fly ac...</td>\n",
              "      <td>0EpgpJCLJmNneFCJOgbH53</td>\n",
              "      <td>Hello, welcome to this listenership. I love Te...</td>\n",
              "      <td>’’’</td>\n",
              "      <td>Hello, welcome to this listenership. I love Te...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hello, welcome to this listenership. I love Te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>159</td>\n",
              "      <td>spotify:show:1FByGzjkh3GjHIJATbWVfw</td>\n",
              "      <td>Aliens Like Us with Rhys Darby</td>\n",
              "      <td>A lighthearted look into all things Extraterre...</td>\n",
              "      <td>Spotify Studios</td>\n",
              "      <td>['en']</td>\n",
              "      <td>https://feeds.megaphone.fm/alienslikeus</td>\n",
              "      <td>spotify:episode:3oXPM2UeBnGHRjnua5igyt</td>\n",
              "      <td>Aliens Like Us with Rhys Darby coming soon to ...</td>\n",
              "      <td>A lighthearted look into all things Extraterre...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_1FByGzjkh3GjHIJATbWVfw</td>\n",
              "      <td>3oXPM2UeBnGHRjnua5igyt</td>\n",
              "      <td>39</td>\n",
              "      <td>A lighthearted look into all things Extraterre...</td>\n",
              "      <td>3oXPM2UeBnGHRjnua5igyt</td>\n",
              "      <td>Greetings earthlings. I'm restar be fellow hum...</td>\n",
              "      <td></td>\n",
              "      <td>Greetings earthlings. I'm restar be fellow hum...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Greetings earthlings. I'm restar be fellow hum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>160</td>\n",
              "      <td>spotify:show:2k2p3GMgBzOyoX1s7zVlob</td>\n",
              "      <td>In Route</td>\n",
              "      <td>IN ROUTE exemplifies our life routes and all t...</td>\n",
              "      <td>Grace Meister</td>\n",
              "      <td>['en']</td>\n",
              "      <td>https://anchor.fm/s/106bcdd4/podcast/rss</td>\n",
              "      <td>spotify:episode:0xPM9iUp8p1y0hd3Js3ADN</td>\n",
              "      <td>Stop #4- Scratch that plan</td>\n",
              "      <td>In this episode of IN ROUTE podcast I talk abo...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_2k2p3GMgBzOyoX1s7zVlob</td>\n",
              "      <td>0xPM9iUp8p1y0hd3Js3ADN</td>\n",
              "      <td>28</td>\n",
              "      <td>In this episode of IN ROUTE podcast I talk abo...</td>\n",
              "      <td>0xPM9iUp8p1y0hd3Js3ADN</td>\n",
              "      <td>What's up? What's up? Welcome back to internal...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What's up? What's up? Welcome back to internal...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What's up? What's up? Welcome back to internal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>194</td>\n",
              "      <td>spotify:show:14jHiFCK3Cl3JFYusju8Kz</td>\n",
              "      <td>Revise - A Level Physics Revision</td>\n",
              "      <td>Let other students help you revise for your A ...</td>\n",
              "      <td>Seneca Learning Revision</td>\n",
              "      <td>['en']</td>\n",
              "      <td>https://anchor.fm/s/a3a4f80/podcast/rss</td>\n",
              "      <td>spotify:episode:0L7N070XuucttXVMDQLyKQ</td>\n",
              "      <td>DC Circuits: Current, Voltage &amp; Resistance 🔌 -...</td>\n",
              "      <td>Emma looks at current, voltage and resistance ...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_14jHiFCK3Cl3JFYusju8Kz</td>\n",
              "      <td>0L7N070XuucttXVMDQLyKQ</td>\n",
              "      <td>59</td>\n",
              "      <td>Emma looks at current, voltage and resistance ...</td>\n",
              "      <td>0L7N070XuucttXVMDQLyKQ</td>\n",
              "      <td>Electrical current is the flow or movement of ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Electrical current is the flow or movement of ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Electrical current is the flow or movement of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>223</td>\n",
              "      <td>spotify:show:5oiyM9QX9mIh4QiAghk9jX</td>\n",
              "      <td>Possibility of Positivity</td>\n",
              "      <td>Welcome to the Positive Place as this channel ...</td>\n",
              "      <td>Chris McPherson</td>\n",
              "      <td>['en-US']</td>\n",
              "      <td>https://anchor.fm/s/71d40a0/podcast/rss</td>\n",
              "      <td>spotify:episode:60Snv1qyJCwXYmOfUrAI8U</td>\n",
              "      <td>Beauty Is More Than Skin Deep</td>\n",
              "      <td>The Positive Place Episode 46: Beauty Is More ...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_5oiyM9QX9mIh4QiAghk9jX</td>\n",
              "      <td>60Snv1qyJCwXYmOfUrAI8U</td>\n",
              "      <td>109</td>\n",
              "      <td>The Positive Place Episode 46: Beauty Is More ...</td>\n",
              "      <td>60Snv1qyJCwXYmOfUrAI8U</td>\n",
              "      <td>What's going on World at am Chris McPherson? A...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What's going on World at am Chris McPherson? A...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What's going on World at am Chris McPherson? A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>250</td>\n",
              "      <td>spotify:show:182zZs65h5VqYF1dLjC7ct</td>\n",
              "      <td>with love, leila</td>\n",
              "      <td>As a young Muslim Black woman I have many opin...</td>\n",
              "      <td>Leila Khedir</td>\n",
              "      <td>['en']</td>\n",
              "      <td>https://anchor.fm/s/12496f44/podcast/rss</td>\n",
              "      <td>spotify:episode:7forP0dPFxLPT2ZLoQjImL</td>\n",
              "      <td>Episode 1: Welcome!</td>\n",
              "      <td>Hi guys! Welcome to the first episode of with ...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_182zZs65h5VqYF1dLjC7ct</td>\n",
              "      <td>7forP0dPFxLPT2ZLoQjImL</td>\n",
              "      <td>40</td>\n",
              "      <td>Hi guys! Welcome to the first episode of with ...</td>\n",
              "      <td>7forP0dPFxLPT2ZLoQjImL</td>\n",
              "      <td>Hello guys and welcome to with love Leela. It'...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hello guys and welcome to with love Leela. It'...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hello guys and welcome to with love Leela. It'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>252</td>\n",
              "      <td>spotify:show:48sAK3XEtbUEXXItmcRdEe</td>\n",
              "      <td>The Keeping Up with the Kardashians Podcast</td>\n",
              "      <td>The Keeping Up with the Kardashians After Show...</td>\n",
              "      <td>AfterBuzz TV</td>\n",
              "      <td>['en-US']</td>\n",
              "      <td>https://anchor.fm/s/6dc2570/podcast/rss</td>\n",
              "      <td>spotify:episode:6PjqerZA54f87tH0XE3zum</td>\n",
              "      <td>Taylor Swift REIGNITES Feud with Kanye!?</td>\n",
              "      <td>Taylor Swift is starting new beef with Kanye. ...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_48sAK3XEtbUEXXItmcRdEe</td>\n",
              "      <td>6PjqerZA54f87tH0XE3zum</td>\n",
              "      <td>44</td>\n",
              "      <td>Taylor Swift is starting new beef with Kanye. ...</td>\n",
              "      <td>6PjqerZA54f87tH0XE3zum</td>\n",
              "      <td>Taylor Swift is starting new beats with Kanye....</td>\n",
              "      <td>’</td>\n",
              "      <td>Taylor Swift is starting new beats with Kanye....</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Taylor Swift is starting new beats with Kanye....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>271</td>\n",
              "      <td>spotify:show:3od2mRbyL3jJUd9GMbZrDs</td>\n",
              "      <td>Bush League Sports</td>\n",
              "      <td>Bush League Sports is a weekly podcast, hosted...</td>\n",
              "      <td>BLS</td>\n",
              "      <td>['en']</td>\n",
              "      <td>https://anchor.fm/s/c9c36a8/podcast/rss</td>\n",
              "      <td>spotify:episode:1qdWhEcLxTq7TZVQYdUOgm</td>\n",
              "      <td>Episode Zero: The Intro</td>\n",
              "      <td>A brief overview of what we are bringing to th...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_3od2mRbyL3jJUd9GMbZrDs</td>\n",
              "      <td>1qdWhEcLxTq7TZVQYdUOgm</td>\n",
              "      <td>36</td>\n",
              "      <td>A brief overview of what we are bringing to th...</td>\n",
              "      <td>1qdWhEcLxTq7TZVQYdUOgm</td>\n",
              "      <td>Welcome to Bush League Sports hola. Hello me h...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Welcome to Bush League Sports hola. Hello me h...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Welcome to Bush League Sports hola. Hello me h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>278</td>\n",
              "      <td>spotify:show:3nP6AbXrTjg4thS7bWWiyH</td>\n",
              "      <td>Bamo Nob Podcast</td>\n",
              "      <td>Challenge yourself. Step it up. Work out 5 min...</td>\n",
              "      <td>Mismark</td>\n",
              "      <td>['en']</td>\n",
              "      <td>https://anchor.fm/s/130406d8/podcast/rss</td>\n",
              "      <td>spotify:episode:1ZYtLraIw8Pg26r6ZaNFKl</td>\n",
              "      <td>Episode 1– Top 10 Fitness Tips</td>\n",
              "      <td>If you dread a long workout, break it into sm...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_3nP6AbXrTjg4thS7bWWiyH</td>\n",
              "      <td>1ZYtLraIw8Pg26r6ZaNFKl</td>\n",
              "      <td>50</td>\n",
              "      <td>If you dread a long workout, break it into sm...</td>\n",
              "      <td>1ZYtLraIw8Pg26r6ZaNFKl</td>\n",
              "      <td>You'd actually need to to to do it like, you k...</td>\n",
              "      <td></td>\n",
              "      <td>You'd actually need to to to do it like, you k...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>You'd actually need to to to do it like, you k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>284</td>\n",
              "      <td>spotify:show:7wFkuE5RCg46a34ml6r9rB</td>\n",
              "      <td>Josh Reads (other people's published) Poetry</td>\n",
              "      <td>In this podcast I read other people's (publish...</td>\n",
              "      <td>Josh Guilar</td>\n",
              "      <td>['en-AU']</td>\n",
              "      <td>https://anchor.fm/s/38eeaec/podcast/rss</td>\n",
              "      <td>spotify:episode:3JwJzd8oBciF0l4tXDrO3J</td>\n",
              "      <td>My Sad Self by Allen Ginsberg</td>\n",
              "      <td>In this episode I read My Sad Self by Allen Gi...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_7wFkuE5RCg46a34ml6r9rB</td>\n",
              "      <td>3JwJzd8oBciF0l4tXDrO3J</td>\n",
              "      <td>29</td>\n",
              "      <td>In this episode I read My Sad Self by Allen Gi...</td>\n",
              "      <td>3JwJzd8oBciF0l4tXDrO3J</td>\n",
              "      <td>In this episode of Josh read other people's pu...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In this episode of Josh read other people's pu...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In this episode of Josh read other people's pu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>368</td>\n",
              "      <td>spotify:show:5ug9KgrPVmvlASTJyWweF2</td>\n",
              "      <td>The Language Tutor Spanish</td>\n",
              "      <td>The Language Tutor Spanish podcast series allo...</td>\n",
              "      <td>Danny Evans</td>\n",
              "      <td>['en']</td>\n",
              "      <td>https://anchor.fm/s/c583548/podcast/rss</td>\n",
              "      <td>spotify:episode:59L6rTh1kEd2lVR9VrtRJU</td>\n",
              "      <td>The Language Tutor Spanish - Episode 46 - Past...</td>\n",
              "      <td>This episode practices the verbs \"to be able t...</td>\n",
              "      <td>...</td>\n",
              "      <td>show_5ug9KgrPVmvlASTJyWweF2</td>\n",
              "      <td>59L6rTh1kEd2lVR9VrtRJU</td>\n",
              "      <td>103</td>\n",
              "      <td>This episode practices the verbs \"to be able t...</td>\n",
              "      <td>59L6rTh1kEd2lVR9VrtRJU</td>\n",
              "      <td>Well, I mean my friends welcome back to the la...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Well, I mean my friends welcome back to the la...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Well, I mean my friends welcome back to the la...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82df3b8e-560d-4384-b969-3f55f95a538f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-82df3b8e-560d-4384-b969-3f55f95a538f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-82df3b8e-560d-4384-b969-3f55f95a538f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Unnamed: 0                             show_uri  \\\n",
              "18           18  spotify:show:6R5ktHYRdfrm1ePuPcHeBe   \n",
              "22           22  spotify:show:5MK1Xk5Yd9ATgQGg7D3P1m   \n",
              "50           50  spotify:show:5mlrDhLAEhaaGszJezXvjF   \n",
              "52           52  spotify:show:4drmkFJHxGvu7X69hkjSUB   \n",
              "53           53  spotify:show:1Ymux9Rk8sPpz7gtKdgeLR   \n",
              "60           60  spotify:show:0NpVLVfKd8mtKlBjmEj0vu   \n",
              "72           72  spotify:show:3c3gzFgW09IjpruwOHoEaC   \n",
              "79           79  spotify:show:6tIYhCPdRlFvNrJcDG9uw4   \n",
              "138         138  spotify:show:5mlrDhLAEhaaGszJezXvjF   \n",
              "150         150  spotify:show:3aFDEGgGGeXX28Sn9ZkAh1   \n",
              "159         159  spotify:show:1FByGzjkh3GjHIJATbWVfw   \n",
              "160         160  spotify:show:2k2p3GMgBzOyoX1s7zVlob   \n",
              "194         194  spotify:show:14jHiFCK3Cl3JFYusju8Kz   \n",
              "223         223  spotify:show:5oiyM9QX9mIh4QiAghk9jX   \n",
              "250         250  spotify:show:182zZs65h5VqYF1dLjC7ct   \n",
              "252         252  spotify:show:48sAK3XEtbUEXXItmcRdEe   \n",
              "271         271  spotify:show:3od2mRbyL3jJUd9GMbZrDs   \n",
              "278         278  spotify:show:3nP6AbXrTjg4thS7bWWiyH   \n",
              "284         284  spotify:show:7wFkuE5RCg46a34ml6r9rB   \n",
              "368         368  spotify:show:5ug9KgrPVmvlASTJyWweF2   \n",
              "\n",
              "                                        show_name  \\\n",
              "18                           Storytime with Jane    \n",
              "22                       The Sweet Sounds of ASMR   \n",
              "50                        Daily Christian Podcast   \n",
              "52                             Affirmations by A.   \n",
              "53                           Guru - The Alchemist   \n",
              "60                              Stoic Meditations   \n",
              "72                        Steven's weekly podcast   \n",
              "79                                     Bunga Mega   \n",
              "138                       Daily Christian Podcast   \n",
              "150                                  TENI PODCAST   \n",
              "159                Aliens Like Us with Rhys Darby   \n",
              "160                                     In Route    \n",
              "194             Revise - A Level Physics Revision   \n",
              "223                     Possibility of Positivity   \n",
              "250                              with love, leila   \n",
              "252   The Keeping Up with the Kardashians Podcast   \n",
              "271                            Bush League Sports   \n",
              "278                              Bamo Nob Podcast   \n",
              "284  Josh Reads (other people's published) Poetry   \n",
              "368                    The Language Tutor Spanish   \n",
              "\n",
              "                                      show_description  \\\n",
              "18   Listen to a new children’s storybook every day...   \n",
              "22   Welcome to the Sweet Sounds Of ASMR for those ...   \n",
              "50   Listen in on how reading the bible every day i...   \n",
              "52   Affirmations by A. delivers powerful affirmati...   \n",
              "53           India market view Nifty Bank nifty Levels   \n",
              "60   Occasional reflections on the wisdom of Ancien...   \n",
              "72   Steven is a thought leader on the transformati...   \n",
              "79   Bunga Mega- known as coach, counselor & book a...   \n",
              "138  Listen in on how reading the bible every day i...   \n",
              "150  There are all sorts of ways to kill a perfectl...   \n",
              "159  A lighthearted look into all things Extraterre...   \n",
              "160  IN ROUTE exemplifies our life routes and all t...   \n",
              "194  Let other students help you revise for your A ...   \n",
              "223  Welcome to the Positive Place as this channel ...   \n",
              "250  As a young Muslim Black woman I have many opin...   \n",
              "252  The Keeping Up with the Kardashians After Show...   \n",
              "271  Bush League Sports is a weekly podcast, hosted...   \n",
              "278  Challenge yourself. Step it up. Work out 5 min...   \n",
              "284  In this podcast I read other people's (publish...   \n",
              "368  The Language Tutor Spanish podcast series allo...   \n",
              "\n",
              "                    publisher   language  \\\n",
              "18                       Jane     ['en']   \n",
              "22          Parris Youngblood     ['en']   \n",
              "50               Paul Dziepak  ['en-US']   \n",
              "52          Affirmations by A     ['en']   \n",
              "53       MyBillion on the web     ['en']   \n",
              "60          Massimo Pigliucci  ['en-US']   \n",
              "72       Steven Van Belleghem  ['nl-BE']   \n",
              "79                 Bunga Mega     ['en']   \n",
              "138              Paul Dziepak  ['en-US']   \n",
              "150        leoshealth podcast     ['en']   \n",
              "159           Spotify Studios     ['en']   \n",
              "160             Grace Meister     ['en']   \n",
              "194  Seneca Learning Revision     ['en']   \n",
              "223           Chris McPherson  ['en-US']   \n",
              "250              Leila Khedir     ['en']   \n",
              "252              AfterBuzz TV  ['en-US']   \n",
              "271                       BLS     ['en']   \n",
              "278                   Mismark     ['en']   \n",
              "284               Josh Guilar  ['en-AU']   \n",
              "368               Danny Evans     ['en']   \n",
              "\n",
              "                                     rss_link  \\\n",
              "18   https://anchor.fm/s/126503d0/podcast/rss   \n",
              "22    https://anchor.fm/s/89553a0/podcast/rss   \n",
              "50    https://anchor.fm/s/2ec6a88/podcast/rss   \n",
              "52    https://anchor.fm/s/389bf18/podcast/rss   \n",
              "53    https://anchor.fm/s/c011628/podcast/rss   \n",
              "60    https://anchor.fm/s/1D80328/podcast/rss   \n",
              "72    https://anchor.fm/s/3693b44/podcast/rss   \n",
              "79    https://anchor.fm/s/8fb7eb4/podcast/rss   \n",
              "138   https://anchor.fm/s/2ec6a88/podcast/rss   \n",
              "150  https://anchor.fm/s/11af33ac/podcast/rss   \n",
              "159   https://feeds.megaphone.fm/alienslikeus   \n",
              "160  https://anchor.fm/s/106bcdd4/podcast/rss   \n",
              "194   https://anchor.fm/s/a3a4f80/podcast/rss   \n",
              "223   https://anchor.fm/s/71d40a0/podcast/rss   \n",
              "250  https://anchor.fm/s/12496f44/podcast/rss   \n",
              "252   https://anchor.fm/s/6dc2570/podcast/rss   \n",
              "271   https://anchor.fm/s/c9c36a8/podcast/rss   \n",
              "278  https://anchor.fm/s/130406d8/podcast/rss   \n",
              "284   https://anchor.fm/s/38eeaec/podcast/rss   \n",
              "368   https://anchor.fm/s/c583548/podcast/rss   \n",
              "\n",
              "                                episode_uri  \\\n",
              "18   spotify:episode:3jMe7zTRTwCm7q7VRqjDoX   \n",
              "22   spotify:episode:0bLEUbqPdsFOEPck4G6FGM   \n",
              "50   spotify:episode:32tNQkYcpgFOmWOrqpvPnp   \n",
              "52   spotify:episode:1sYSra5nkgTMtnWuTq3yDA   \n",
              "53   spotify:episode:7AUiSbcR4Rp8RgJvAS0I8i   \n",
              "60   spotify:episode:7Iq72L6XjQBFDZncbuO70i   \n",
              "72   spotify:episode:5ptpXqYYvSw3TItmyzDZB6   \n",
              "79   spotify:episode:3ZXWygw71BQZxOw5Zpy49o   \n",
              "138  spotify:episode:5njlEFALN6ASvhJ0h3tD6g   \n",
              "150  spotify:episode:0EpgpJCLJmNneFCJOgbH53   \n",
              "159  spotify:episode:3oXPM2UeBnGHRjnua5igyt   \n",
              "160  spotify:episode:0xPM9iUp8p1y0hd3Js3ADN   \n",
              "194  spotify:episode:0L7N070XuucttXVMDQLyKQ   \n",
              "223  spotify:episode:60Snv1qyJCwXYmOfUrAI8U   \n",
              "250  spotify:episode:7forP0dPFxLPT2ZLoQjImL   \n",
              "252  spotify:episode:6PjqerZA54f87tH0XE3zum   \n",
              "271  spotify:episode:1qdWhEcLxTq7TZVQYdUOgm   \n",
              "278  spotify:episode:1ZYtLraIw8Pg26r6ZaNFKl   \n",
              "284  spotify:episode:3JwJzd8oBciF0l4tXDrO3J   \n",
              "368  spotify:episode:59L6rTh1kEd2lVR9VrtRJU   \n",
              "\n",
              "                                          episode_name  \\\n",
              "18                              The Gruffalo’s Child 📚   \n",
              "22                                      ASMR of Things   \n",
              "50                            Reflect on your journey.   \n",
              "52                                    34. Own Your Day   \n",
              "53   India market view by Guru - The alchemist 27th...   \n",
              "60                Be careful with the company you keep   \n",
              "72   Steven's week 134: News about Facebook, stream...   \n",
              "79   Marriage & Family : Mau Mewariskan Pernikahan ...   \n",
              "138                      I see you in your distress...   \n",
              "150      EP 00001 Car maintenance and servicing basics   \n",
              "159  Aliens Like Us with Rhys Darby coming soon to ...   \n",
              "160                        Stop #4- Scratch that plan    \n",
              "194  DC Circuits: Current, Voltage & Resistance 🔌 -...   \n",
              "223                      Beauty Is More Than Skin Deep   \n",
              "250                                Episode 1: Welcome!   \n",
              "252           Taylor Swift REIGNITES Feud with Kanye!?   \n",
              "271                            Episode Zero: The Intro   \n",
              "278                   Episode 1– Top 10 Fitness Tips     \n",
              "284                      My Sad Self by Allen Ginsberg   \n",
              "368  The Language Tutor Spanish - Episode 46 - Past...   \n",
              "\n",
              "                                   episode_description  ...  \\\n",
              "18   The little Gruffalo goes on an adventure throu...  ...   \n",
              "22   In this ASMR podcast I will be putting makeup ...  ...   \n",
              "50   As we start reading through the book of Deuter...  ...   \n",
              "52   This episode was created to help you make a co...  ...   \n",
              "53   Nifty, bank nifty views, levels. Last pod cast...  ...   \n",
              "60   We should live with the quietest and easiest-t...  ...   \n",
              "72   Hey guys, welcome to a new episode! Facebook a...  ...   \n",
              "79   Kita adalah produk nilai dan budaya pernikahan...  ...   \n",
              "138  The Lord sees his people while they were in di...  ...   \n",
              "150  If you were about to get on a plane and fly ac...  ...   \n",
              "159  A lighthearted look into all things Extraterre...  ...   \n",
              "160  In this episode of IN ROUTE podcast I talk abo...  ...   \n",
              "194  Emma looks at current, voltage and resistance ...  ...   \n",
              "223  The Positive Place Episode 46: Beauty Is More ...  ...   \n",
              "250  Hi guys! Welcome to the first episode of with ...  ...   \n",
              "252  Taylor Swift is starting new beef with Kanye. ...  ...   \n",
              "271  A brief overview of what we are bringing to th...  ...   \n",
              "278   If you dread a long workout, break it into sm...  ...   \n",
              "284  In this episode I read My Sad Self by Allen Gi...  ...   \n",
              "368  This episode practices the verbs \"to be able t...  ...   \n",
              "\n",
              "            show_filename_prefix episode_filename_prefix word_count  \\\n",
              "18   show_6R5ktHYRdfrm1ePuPcHeBe  3jMe7zTRTwCm7q7VRqjDoX         59   \n",
              "22   show_5MK1Xk5Yd9ATgQGg7D3P1m  0bLEUbqPdsFOEPck4G6FGM         43   \n",
              "50   show_5mlrDhLAEhaaGszJezXvjF  32tNQkYcpgFOmWOrqpvPnp         50   \n",
              "52   show_4drmkFJHxGvu7X69hkjSUB  1sYSra5nkgTMtnWuTq3yDA         85   \n",
              "53   show_1Ymux9Rk8sPpz7gtKdgeLR  7AUiSbcR4Rp8RgJvAS0I8i         42   \n",
              "60   show_0NpVLVfKd8mtKlBjmEj0vu  7Iq72L6XjQBFDZncbuO70i         36   \n",
              "72   show_3c3gzFgW09IjpruwOHoEaC  5ptpXqYYvSw3TItmyzDZB6         38   \n",
              "79   show_6tIYhCPdRlFvNrJcDG9uw4  3ZXWygw71BQZxOw5Zpy49o         46   \n",
              "138  show_5mlrDhLAEhaaGszJezXvjF  5njlEFALN6ASvhJ0h3tD6g         27   \n",
              "150  show_3aFDEGgGGeXX28Sn9ZkAh1  0EpgpJCLJmNneFCJOgbH53         70   \n",
              "159  show_1FByGzjkh3GjHIJATbWVfw  3oXPM2UeBnGHRjnua5igyt         39   \n",
              "160  show_2k2p3GMgBzOyoX1s7zVlob  0xPM9iUp8p1y0hd3Js3ADN         28   \n",
              "194  show_14jHiFCK3Cl3JFYusju8Kz  0L7N070XuucttXVMDQLyKQ         59   \n",
              "223  show_5oiyM9QX9mIh4QiAghk9jX  60Snv1qyJCwXYmOfUrAI8U        109   \n",
              "250  show_182zZs65h5VqYF1dLjC7ct  7forP0dPFxLPT2ZLoQjImL         40   \n",
              "252  show_48sAK3XEtbUEXXItmcRdEe  6PjqerZA54f87tH0XE3zum         44   \n",
              "271  show_3od2mRbyL3jJUd9GMbZrDs  1qdWhEcLxTq7TZVQYdUOgm         36   \n",
              "278  show_3nP6AbXrTjg4thS7bWWiyH  1ZYtLraIw8Pg26r6ZaNFKl         50   \n",
              "284  show_7wFkuE5RCg46a34ml6r9rB  3JwJzd8oBciF0l4tXDrO3J         29   \n",
              "368  show_5ug9KgrPVmvlASTJyWweF2  59L6rTh1kEd2lVR9VrtRJU        103   \n",
              "\n",
              "                                       cleaned_ep_desc  \\\n",
              "18   The little Gruffalo goes on an adventure throu...   \n",
              "22   In this ASMR podcast I will be putting makeup ...   \n",
              "50   As we start reading through the book of Deuter...   \n",
              "52   This episode was created to help you make a co...   \n",
              "53   Nifty, bank nifty views, levels. Last pod cast...   \n",
              "60   We should live with the quietest and easiest-t...   \n",
              "72   Hey guys, welcome to a new episode! Facebook a...   \n",
              "79   Kita adalah produk nilai dan budaya pernikahan...   \n",
              "138  The Lord sees his people while they were in di...   \n",
              "150  If you were about to get on a plane and fly ac...   \n",
              "159  A lighthearted look into all things Extraterre...   \n",
              "160  In this episode of IN ROUTE podcast I talk abo...   \n",
              "194  Emma looks at current, voltage and resistance ...   \n",
              "223  The Positive Place Episode 46: Beauty Is More ...   \n",
              "250  Hi guys! Welcome to the first episode of with ...   \n",
              "252  Taylor Swift is starting new beef with Kanye. ...   \n",
              "271  A brief overview of what we are bringing to th...   \n",
              "278   If you dread a long workout, break it into sm...   \n",
              "284  In this episode I read My Sad Self by Allen Gi...   \n",
              "368  This episode practices the verbs \"to be able t...   \n",
              "\n",
              "                         ID  \\\n",
              "18   3jMe7zTRTwCm7q7VRqjDoX   \n",
              "22   0bLEUbqPdsFOEPck4G6FGM   \n",
              "50   32tNQkYcpgFOmWOrqpvPnp   \n",
              "52   1sYSra5nkgTMtnWuTq3yDA   \n",
              "53   7AUiSbcR4Rp8RgJvAS0I8i   \n",
              "60   7Iq72L6XjQBFDZncbuO70i   \n",
              "72   5ptpXqYYvSw3TItmyzDZB6   \n",
              "79   3ZXWygw71BQZxOw5Zpy49o   \n",
              "138  5njlEFALN6ASvhJ0h3tD6g   \n",
              "150  0EpgpJCLJmNneFCJOgbH53   \n",
              "159  3oXPM2UeBnGHRjnua5igyt   \n",
              "160  0xPM9iUp8p1y0hd3Js3ADN   \n",
              "194  0L7N070XuucttXVMDQLyKQ   \n",
              "223  60Snv1qyJCwXYmOfUrAI8U   \n",
              "250  7forP0dPFxLPT2ZLoQjImL   \n",
              "252  6PjqerZA54f87tH0XE3zum   \n",
              "271  1qdWhEcLxTq7TZVQYdUOgm   \n",
              "278  1ZYtLraIw8Pg26r6ZaNFKl   \n",
              "284  3JwJzd8oBciF0l4tXDrO3J   \n",
              "368  59L6rTh1kEd2lVR9VrtRJU   \n",
              "\n",
              "                                            Transcript ep_desc_weirdchars  \\\n",
              "18   Welcome to story time. Today. We're going to b...          ❄️🐁🌳🌲🐿🦔’📚   \n",
              "22   Guys, do you want to hear something amazing? A...                      \n",
              "50   It's pretty much the middle of the year right ...                NaN   \n",
              "52   Hey guys, so if you haven't heard about anchor...                  ·   \n",
              "53   Good morning friends. Welcome to report cause ...                NaN   \n",
              "60   Welcome to the stoic meditations podcast where...                NaN   \n",
              "72   Hey guys. Thanks for watching this new episode...                NaN   \n",
              "79   Hello, Kappa slow mode that only podcast Mega ...                NaN   \n",
              "138  Well, Happy Father's day. It's all you awesome...                NaN   \n",
              "150  Hello, welcome to this listenership. I love Te...                ’’’   \n",
              "159  Greetings earthlings. I'm restar be fellow hum...                      \n",
              "160  What's up? What's up? Welcome back to internal...                NaN   \n",
              "194  Electrical current is the flow or movement of ...                NaN   \n",
              "223  What's going on World at am Chris McPherson? A...                NaN   \n",
              "250  Hello guys and welcome to with love Leela. It'...                NaN   \n",
              "252  Taylor Swift is starting new beats with Kanye....                ’     \n",
              "271  Welcome to Bush League Sports hola. Hello me h...                NaN   \n",
              "278  You'd actually need to to to do it like, you k...                      \n",
              "284  In this episode of Josh read other people's pu...                NaN   \n",
              "368  Well, I mean my friends welcome back to the la...                NaN   \n",
              "\n",
              "                                    ep_desc_unidecoded Transcript_weirdchars  \\\n",
              "18   Welcome to story time. Today. We're going to b...                   NaN   \n",
              "22   Guys, do you want to hear something amazing? A...                   NaN   \n",
              "50   It's pretty much the middle of the year right ...                   NaN   \n",
              "52   Hey guys, so if you haven't heard about anchor...                   NaN   \n",
              "53   Good morning friends. Welcome to report cause ...                   NaN   \n",
              "60   Welcome to the stoic meditations podcast where...                   NaN   \n",
              "72   Hey guys. Thanks for watching this new episode...                   NaN   \n",
              "79   Hello, Kappa slow mode that only podcast Mega ...                   NaN   \n",
              "138  Well, Happy Father's day. It's all you awesome...                   NaN   \n",
              "150  Hello, welcome to this listenership. I love Te...                   NaN   \n",
              "159  Greetings earthlings. I'm restar be fellow hum...                   NaN   \n",
              "160  What's up? What's up? Welcome back to internal...                   NaN   \n",
              "194  Electrical current is the flow or movement of ...                   NaN   \n",
              "223  What's going on World at am Chris McPherson? A...                   NaN   \n",
              "250  Hello guys and welcome to with love Leela. It'...                   NaN   \n",
              "252  Taylor Swift is starting new beats with Kanye....                   NaN   \n",
              "271  Welcome to Bush League Sports hola. Hello me h...                   NaN   \n",
              "278  You'd actually need to to to do it like, you k...                   NaN   \n",
              "284  In this episode of Josh read other people's pu...                   NaN   \n",
              "368  Well, I mean my friends welcome back to the la...                   NaN   \n",
              "\n",
              "                                 Transcript_unidecoded  \n",
              "18   Welcome to story time. Today. We're going to b...  \n",
              "22   Guys, do you want to hear something amazing? A...  \n",
              "50   It's pretty much the middle of the year right ...  \n",
              "52   Hey guys, so if you haven't heard about anchor...  \n",
              "53   Good morning friends. Welcome to report cause ...  \n",
              "60   Welcome to the stoic meditations podcast where...  \n",
              "72   Hey guys. Thanks for watching this new episode...  \n",
              "79   Hello, Kappa slow mode that only podcast Mega ...  \n",
              "138  Well, Happy Father's day. It's all you awesome...  \n",
              "150  Hello, welcome to this listenership. I love Te...  \n",
              "159  Greetings earthlings. I'm restar be fellow hum...  \n",
              "160  What's up? What's up? Welcome back to internal...  \n",
              "194  Electrical current is the flow or movement of ...  \n",
              "223  What's going on World at am Chris McPherson? A...  \n",
              "250  Hello guys and welcome to with love Leela. It'...  \n",
              "252  Taylor Swift is starting new beats with Kanye....  \n",
              "271  Welcome to Bush League Sports hola. Hello me h...  \n",
              "278  You'd actually need to to to do it like, you k...  \n",
              "284  In this episode of Josh read other people's pu...  \n",
              "368  Well, I mean my friends welcome back to the la...  \n",
              "\n",
              "[20 rows x 21 columns]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_full.head(100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D5hS4NZP4_Z"
      },
      "source": [
        "# Pegasus Model - PEGASUS model and tokenizer with chuck size of 150 reading first 10 and last 2 lines of the text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQhLWMi09bUr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the PEGASUS model and tokenizer with chuck size of 150 reading first 10 and last 2 lines of the text \n",
        "summary_model = 'google/pegasus-cnn_dailymail'\n",
        "summary_tokenizer = PegasusTokenizer.from_pretrained(summary_model)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(summary_model)\n",
        "\n",
        "def summarize_text(text):\n",
        "    # Load the PEGASUS model and tokenizer\n",
        "    summary_model = 'google/pegasus-xsum'\n",
        "    summary_tokenizer = PegasusTokenizer.from_pretrained(summary_model)\n",
        "    model = PegasusForConditionalGeneration.from_pretrained(summary_model)\n",
        "\n",
        "    # Select the first 10 and last 2 lines of the text\n",
        "    lines = text.split('\\n')\n",
        "    selected_lines = lines[:10] + lines[-2:]\n",
        "\n",
        "    # Join the selected lines into a single string\n",
        "    text = '\\n'.join(selected_lines)\n",
        "\n",
        "    # Encode the text into tokens\n",
        "    tokens = summary_tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    # Split the tokens into chunks of up to 150 tokens\n",
        "    chunk_size = 150\n",
        "    chunks = [tokens[i:i+chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
        "\n",
        "    # Generate a summary for each chunk\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        input_ids = torch.tensor(chunk).unsqueeze(0)\n",
        "        summary_ids = model.generate(input_ids, num_beams=4, max_length=150, early_stopping=True)\n",
        "        summary = summary_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    # Combine the summaries into a single string\n",
        "    summary = ' '.join(summaries)\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def evaluate_model(df):\n",
        "    # Initialize the ROUGE scorer\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # Select the first 150 transcripts\n",
        "    df = df.head(10)\n",
        "\n",
        "    # Calculate the ROUGE scores for each row in the DataFrame\n",
        "    rouge_scores = []\n",
        "    for index, row in df.iterrows():\n",
        "        if not row['Transcript']:  # Check if transcript column is empty\n",
        "            continue\n",
        "        # Generate the summary\n",
        "        summary = summarize_text(row['Transcript'])\n",
        "        \n",
        "        # Calculate the ROUGE scores\n",
        "        scores = scorer.score(row['episode_description'], summary)\n",
        "        rouge_scores.append(scores)\n",
        "\n",
        "    # Calculate the average ROUGE scores\n",
        "    avg_scores = {}\n",
        "    for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
        "        avg_scores[metric] = sum(score[metric].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "\n",
        "    return avg_scores\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaAQ-9Vs_aJq",
        "outputId": "02432acf-91c8-4209-be0c-2d126345cf9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'rouge1': 0.10361759237705057, 'rouge2': 0.017757106981433783, 'rougeL': 0.06659571912393004}\n"
          ]
        }
      ],
      "source": [
        "scores = evaluate_model(selected_df)\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIjrGMRt7KqJ"
      },
      "outputs": [],
      "source": [
        "selected_df.head(100)\n",
        "input_texts = selected_df[\"Transcript\"].tolist()[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KvUFgZZQJqt"
      },
      "source": [
        "# Pegasus Model - With error handling and trouble shooting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u5nDVnOk_a0"
      },
      "outputs": [],
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, pipeline\n",
        "\n",
        "\n",
        "# Load the Pegasus model and tokenizer\n",
        "model_name = \"google/pegasus-xsum\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Define the summarization pipeline\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    framework=\"pt\",\n",
        "    device=0,\n",
        ")\n",
        "\n",
        "# Read the input text from a file and summarize each transcript\n",
        "input_df = pd.read_csv(\"/content/gdrive/MyDrive/Spotify/subsets/500/train.csv\")\n",
        "for i in range(len(input_df)):\n",
        "    # Read the transcript\n",
        "    transcript = input_df.loc[i, \"Transcript\"]\n",
        "    \n",
        "    # Generate the summary using Pegasus with a sliding window\n",
        "    max_chunk = 512\n",
        "    stride = 256\n",
        "    output_text = \"\"\n",
        "    print(\"Length of transcript\", len(transcript))\n",
        "    print(\"Transcript\",transcript)\n",
        "    for j in range(0, len(transcript), stride):\n",
        "        chunk = transcript[j:j + max_chunk]\n",
        "        if len(chunk) < max_chunk:\n",
        "            break\n",
        "        # Split the chunk into sentences and tokenize them\n",
        "        sentences = chunk.split(\".\")\n",
        "        encoded_sentences = tokenizer(sentences, padding=True, truncation=True, max_length=max_chunk, return_tensors=\"pt\")\n",
        "        input_ids = encoded_sentences[\"input_ids\"].to(\"cuda\")\n",
        "        attention_mask = encoded_sentences[\"attention_mask\"].to(\"cuda\")\n",
        "        try:\n",
        "            # Generate the summary for the current chunk\n",
        "            summary = summarizer(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=60,\n",
        "                min_length=10,\n",
        "                do_sample=False,\n",
        "            )\n",
        "            # Decode the summary and append it to the output text\n",
        "            summary_text = tokenizer.decode(summary[0][\"summary_text\"])\n",
        "            output_text += summary_text + \" \"\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred while summarizing transcript {i}: {e}\")\n",
        "    \n",
        "    # Write the summary to an output file\n",
        "    output_dir = \"/content/gdrive/MyDrive/Spotify/subsets/500/summaries\" # specify the absolute path to the output directory\n",
        "    os.makedirs(output_dir, exist_ok=True) # create the output directory if it does not exist\n",
        "    output_filename = os.path.join(output_dir, f\"summary_{i}.txt\") # specify the absolute path to the output file\n",
        "    with open(output_filename, \"w\") as f:\n",
        "        f.write(output_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbmbC1G9l2PA"
      },
      "outputs": [],
      "source": [
        "#Read one file and write to the summaries directory train \n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, SummarizationPipeline\n",
        "\n",
        "# Load the Pegasus model and tokenizer\n",
        "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
        "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
        "\n",
        "# Define functions to read the transcript, summarize it, and write the summary to a file\n",
        "def read_transcript(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    transcript = df['Transcript'][0]\n",
        "    return transcript\n",
        "\n",
        "def summarize_transcript(transcript, chunk_size=150, sliding_window=400):\n",
        "    summarizer = SummarizationPipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "    chunks = [transcript[i:i+chunk_size] for i in range(0, len(transcript), sliding_window)]\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
        "        summaries.append(summary)\n",
        "    summary = ' '.join(summaries)\n",
        "    return summary\n",
        "\n",
        "def write_summary_to_file(summary, file_path):\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write(summary)\n",
        "\n",
        "# Call the functions to read the transcript, summarize it, and write the summary to a file\n",
        "transcript = read_transcript('/content/gdrive/MyDrive/Spotify/subsets/500/train.csv')\n",
        "summary = summarize_transcript(transcript, chunk_size=150, sliding_window=400)\n",
        "write_summary_to_file(summary, '/content/gdrive/MyDrive/Spotify/subsets/500/summaries/train.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6hP-QTUvqGt",
        "outputId": "c134dcde-a4bc-435e-d890-8f786aab5762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarized transcript 1/2 and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/transcript_1.txt\n",
            "Summarized transcript 2/2 and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/transcript_2.txt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, SummarizationPipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Pegasus model and tokenizer\n",
        "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
        "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
        "\n",
        "# Define functions to read the transcript, summarize it, and write the summary to a file\n",
        "def read_transcript(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    transcript = df['Transcript'][0]\n",
        "    return transcript\n",
        "\n",
        "def summarize_transcript(transcript, filename, file_path, chunk_size=150, sliding_window=400):\n",
        "    summarizer = SummarizationPipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "    chunks = [transcript[i:i+chunk_size] for i in range(0, len(transcript), sliding_window)]\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
        "        summaries.append(summary)\n",
        "    summary = ' '.join(summaries)\n",
        "    write_summary_to_file(summary, filename, file_path)\n",
        "    return summary\n",
        "\n",
        "def write_summary_to_file(summary, filename, file_path):\n",
        "    with open(file_path + filename + '.txt', 'w') as f:\n",
        "        f.write(summary)\n",
        "\n",
        "# Define start and end indices for the transcripts to summarize\n",
        "start_idx = 1\n",
        "end_idx = 2\n",
        "\n",
        "# Loop over the specified range of transcripts and summarize each one\n",
        "for i in range(start_idx - 1, end_idx):\n",
        "    file_path = '/content/gdrive/MyDrive/Spotify/subsets/500/train.csv'\n",
        "    filename = f'transcript_{i+1}'\n",
        "    transcript = read_transcript(file_path)\n",
        "    summary = summarize_transcript(transcript, filename, '/content/gdrive/MyDrive/Spotify/subsets/500/summaries/', chunk_size=150, sliding_window=400)\n",
        "    print(f'Summarized transcript {i+1}/{end_idx-start_idx+1} and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/{filename}.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja5wZy4U0Ekg",
        "outputId": "a815c0b1-4878-4fe4-a696-c46a805871ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarized transcript 126/11 and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/transcript_126.csv\n",
            "Summarized transcript 127/11 and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/transcript_127.csv\n",
            "Summarized transcript 128/11 and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/transcript_128.csv\n",
            "Summarized transcript 129/11 and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/transcript_129.csv\n",
            "Summarized transcript 130/11 and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/transcript_130.csv\n",
            "Summarized transcript 131/11 and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/transcript_131.csv\n",
            "Summarized transcript 132/11 and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/transcript_132.csv\n",
            "Summarized transcript 133/11 and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/transcript_133.csv\n",
            "Summarized transcript 134/11 and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/transcript_134.csv\n",
            "Summarized transcript 135/11 and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/transcript_135.csv\n",
            "Summarized transcript 136/11 and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/transcript_136.csv\n"
          ]
        }
      ],
      "source": [
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, SummarizationPipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Pegasus model and tokenizer\n",
        "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
        "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
        "\n",
        "# Define functions to read the transcript, summarize it, and write the summary to a file\n",
        "def read_transcript(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    transcript = df['Transcript'][0]\n",
        "    id = df['ID'][0]\n",
        "    return transcript, id\n",
        "\n",
        "def summarize_transcript(transcript, id, filename, file_path, chunk_size=150, sliding_window=400):\n",
        "    summarizer = SummarizationPipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "    chunks = [transcript[i:i+chunk_size] for i in range(0, len(transcript), sliding_window)]\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
        "        summaries.append(summary)\n",
        "    summary = ' '.join(summaries)\n",
        "    write_summary_to_file(summary, id, filename, file_path)\n",
        "    return summary\n",
        "\n",
        "def write_summary_to_file(summary, id, filename, file_path):\n",
        "    df = pd.DataFrame({\n",
        "        'ID': [id],\n",
        "        'Summary': [summary]\n",
        "    })\n",
        "    df.to_csv(file_path + filename + '.csv', index=False)\n",
        "\n",
        "# Define start and end indices for the transcripts to summarize\n",
        "start_idx = 126\n",
        "end_idx = 136\n",
        "\n",
        "# Loop over the specified range of transcripts and summarize each one\n",
        "for i in range(start_idx - 1, end_idx):\n",
        "    file_path = '/content/gdrive/MyDrive/Spotify/subsets/500/train.csv'\n",
        "    filename = f'transcript_{i+1}'\n",
        "    transcript, id = read_transcript(file_path)\n",
        "    summary = summarize_transcript(transcript, id, filename, '/content/gdrive/MyDrive/Spotify/subsets/500/summaries/', chunk_size=150, sliding_window=400)\n",
        "    print(f'Summarized transcript {i+1}/{end_idx-start_idx+1} and saved summary to /content/gdrive/MyDrive/Spotify/subsets/500/summaries/{filename}.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsSLFdmN2j9e"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, pipeline\n",
        "\n",
        "# Load the Pegasus models and tokenizer for extractive and abstractive summarization\n",
        "tokenizer_ext = PegasusTokenizer.from_pretrained('sshleifer/distill-pegasus-xsum-16-4', max_length=512,  device='cuda')\n",
        "model_ext = PegasusForConditionalGeneration.from_pretrained('sshleifer/distill-pegasus-xsum-16-4', max_length=512)\n",
        "\n",
        "tokenizer_abs = PegasusTokenizer.from_pretrained('google/pegasus-large', max_length=1024,   device='cuda')\n",
        "model_abs = PegasusForConditionalGeneration.from_pretrained('google/pegasus-large',max_length=1024)\n",
        "\n",
        "# Function to generate an extractive summary using sliding window with a frame size of 400 and chunk size of 150\n",
        "def generate_extractive_summary(transcript, frame_size=400, chunk_size=150):\n",
        "    summarizer = pipeline('summarization', model=model_ext, tokenizer=tokenizer_ext, device=0)\n",
        "    num_frames = int(math.ceil(len(transcript) / float(frame_size)))\n",
        "    summary = \"\"\n",
        "    for i in range(num_frames):\n",
        "        start = max(0, i * frame_size - chunk_size//2)\n",
        "        end = min(len(transcript), (i+1)*frame_size + chunk_size//2)\n",
        "        chunk = transcript[start:end]\n",
        "        summary += summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
        "    return summary\n",
        "\n",
        "# Function to generate an abstractive summary using sliding window with a frame size of 400 and chunk size of 150\n",
        "def generate_abstractive_summary(transcript, frame_size=400, chunk_size=150):\n",
        "    summarizer = pipeline('summarization', model=model_abs, tokenizer=tokenizer_abs, device=0)\n",
        "    num_frames = int(math.ceil(len(transcript) / float(frame_size)))\n",
        "    summary = \"\"\n",
        "    for i in range(num_frames):\n",
        "        start = max(0, i * frame_size - chunk_size//2)\n",
        "        end = min(len(transcript), (i+1)*frame_size + chunk_size//2)\n",
        "        chunk = transcript[start:end]\n",
        "        summary += summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
        "    return summary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define a function to read the transcript from a CSV file and generate summaries\n",
        "def generate_summaries(file_path, start_idx, end_idx, frame_size=400, chunk_size=150):\n",
        "    df = pd.read_csv(file_path)\n",
        "    processed = 0  # Initialize the number of transcripts processed\n",
        "    for idx in range(start_idx, end_idx):\n",
        "        transcript = df['Transcript'][idx]\n",
        "        id = df['ID'][idx]\n",
        "        num_frames = int(math.ceil(len(transcript) / float(frame_size)))\n",
        "        extractive_summary = \"\"\n",
        "        abstractive_summary = \"\"\n",
        "        for i in range(num_frames):\n",
        "            start = max(0, i * frame_size - chunk_size//2)\n",
        "            end = min(len(transcript), (i+1)*frame_size + chunk_size//2)\n",
        "            chunk = transcript[start:end]\n",
        "            extractive_summary += generate_extractive_summary(chunk)\n",
        "        abstractive_summary = generate_abstractive_summary(extractive_summary)\n",
        "        summary = {\"ID\": id, \"Extractive_Summary\": extractive_summary, \"Abstractive_Summary\": abstractive_summary}\n",
        "        # Write each summary and ID to individual JSON files\n",
        "        with open(f\"/content/gdrive/MyDrive/Spotify/subsets/500/Summary_hybrid_Pegasus/summary_{id}.json\", 'w') as f:\n",
        "            json.dump(summary, f)\n",
        "        processed += 1  # Increment the number of transcripts processed\n",
        "        print(f\"Processed {processed} out of {end_idx-start_idx} transcripts\")  # Print the progress update\n",
        "    print(f\"Generated and saved summaries for transcripts with IDs between {start_idx} and {end_idx}\")\n",
        "\n",
        "\n",
        "file_path='/content/gdrive/MyDrive/Spotify/subsets/500/train.csv'\n",
        "\n",
        "# Define start and end indices for the transcripts to summarize\n",
        "start_idx = 11\n",
        "end_idx = 20\n",
        "\n",
        "generate_summaries(file_path, start_idx, end_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bpUXVsJLoH8",
        "outputId": "20b6f408-69cc-4cfd-9908-7b0f47030412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 1 out of 9 transcripts\n",
            "Processed 2 out of 9 transcripts\n",
            "Processed 3 out of 9 transcripts\n",
            "Processed 4 out of 9 transcripts\n",
            "Processed 5 out of 9 transcripts\n"
          ]
        }
      ],
      "source": [
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, pipeline\n",
        "import math\n",
        "\n",
        "# Load the Pegasus models and tokenizer for extractive and abstractive summarization\n",
        "tokenizer_ext = PegasusTokenizer.from_pretrained('sshleifer/distill-pegasus-xsum-16-4', max_length=512, device='cuda')\n",
        "model_ext = PegasusForConditionalGeneration.from_pretrained('sshleifer/distill-pegasus-xsum-16-4', max_length=512)\n",
        "\n",
        "tokenizer_abs = PegasusTokenizer.from_pretrained('google/pegasus-large', max_length=1024, device='cuda')\n",
        "model_abs = PegasusForConditionalGeneration.from_pretrained('google/pegasus-large', max_length=1024)\n",
        "\n",
        "# Define a function to generate an extractive summary for a given chunk of text\n",
        "def generate_extractive_summary(transcript):\n",
        "    summarizer = pipeline('summarization', model=model_ext, tokenizer=tokenizer_ext, device=0)\n",
        "    max_chunk_size = 500\n",
        "    chunks = [transcript[i:i+max_chunk_size] for i in range(0, len(transcript), max_chunk_size)]\n",
        "    summary = ''\n",
        "    for chunk in chunks:\n",
        "        summary += summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
        "    return summary\n",
        "\n",
        "# Define a function to generate an abstractive  summary for a given chunk of text\n",
        "def generate_abstractive_summary(transcript):\n",
        "    summarizer = pipeline('summarization', model=model_abs, tokenizer=tokenizer_abs, device=0)\n",
        "    max_chunk_size = 500\n",
        "    chunks = [transcript[i:i+max_chunk_size] for i in range(0, len(transcript), max_chunk_size)]\n",
        "    summary = ''\n",
        "    for chunk in chunks:\n",
        "        summary += summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
        "    return summary\n",
        "\n",
        "\n",
        "# Define a function to read the transcript from a CSV file and generate summaries\n",
        "def generate_summaries(file_path, start_idx, end_idx, frame_size=400, chunk_size=150, stride_gap=-0.5):\n",
        "    df = pd.read_csv(file_path)\n",
        "    processed = 0  # Initialize the number of transcripts processed\n",
        "    for idx in range(start_idx, end_idx):\n",
        "        transcript = df['Transcript'][idx]\n",
        "        id = df['ID'][idx]\n",
        "        num_frames = int(math.ceil(len(transcript) / float(frame_size)))\n",
        "        combined_summary = \"\"\n",
        "        for i in range(num_frames):\n",
        "            start = max(0, int(i * frame_size + stride_gap * chunk_size) - chunk_size//2)\n",
        "            end = min(len(transcript), int((i+1)*frame_size + stride_gap * chunk_size) + chunk_size//2)\n",
        "            chunk = transcript[start:end]\n",
        "            summary = generate_extractive_summary(chunk)\n",
        "            combined_summary += summary + \" \"\n",
        "        final_summary = generate_abstractive_summary(combined_summary)\n",
        "        summary = {\"ID\": id, \"Extractive_Summary\": combined_summary, \"Abstractive_Summary\": final_summary}\n",
        "        # Write each summary and ID to individual JSON files\n",
        "        with open(\"/content/gdrive/MyDrive/Spotify/subsets/500/Summary_hybrid_Pegasus/\"f\"summary_{id}.json\", 'w') as f:\n",
        "            json.dump(summary, f)\n",
        "        processed += 1  # Increment the number of transcripts processed\n",
        "        print(f\"Processed {processed} out of {end_idx-start_idx} transcripts\")  # Print the progress update\n",
        "    print(f\"Generated and saved summaries for transcripts with IDs between {start_idx} and {end_idx}\")\n",
        "\n",
        "\n",
        "file_path='/content/gdrive/MyDrive/Spotify/subsets/500/train.csv'\n",
        "\n",
        "# Define start and end indices for the transcripts to summarize\n",
        "start_idx = 11\n",
        "end_idx = 20\n",
        "\n",
        "generate_summaries(file_path, start_idx, end_idx)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm6_BulsQbFj"
      },
      "source": [
        "# Pegasus Extractive Model Layer 1 with Rouge Scores  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGhngJsY1Wfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d0d455-d70b-473a-ebf0-f00d74eab325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1 out of 1 transcripts\n",
            "Generated and saved extractive summaries for transcripts with IDs between 0 and 1\n"
          ]
        }
      ],
      "source": [
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, pipeline\n",
        "import math\n",
        "from rouge import Rouge\n",
        "\n",
        "# Load the Pegasus models and tokenizer for extractive summarization\n",
        "tokenizer_ext = PegasusTokenizer.from_pretrained('google/pegasus-large', max_length=1024, device='cuda')\n",
        "model_ext = PegasusForConditionalGeneration.from_pretrained('google/pegasus-large', max_length=1024)\n",
        "\n",
        "# Define a function to generate an extractive summary for a given chunk of text\n",
        "def generate_extractive_summary(transcript):\n",
        "    summarizer = pipeline('summarization', model=model_ext, tokenizer=tokenizer_ext, device=0)\n",
        "    max_chunk_size = 8192\n",
        "    chunks = [transcript[i:i+max_chunk_size] for i in range(0, len(transcript), max_chunk_size)]\n",
        "    summary = ''\n",
        "    for chunk in chunks:\n",
        "        summary += summarizer(chunk, max_length=150, min_length=40, do_sample=False, no_repeat_ngram_size=3)[0]['summary_text']\n",
        "    return summary\n",
        "\n",
        "# Define a function to calculate the Rouge score for a given summary and reference summary\n",
        "def calculate_rouge(summary, reference_summary):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(summary, reference_summary)[0]\n",
        "    return scores\n",
        "\n",
        "# Define a function to read the transcript from a CSV file and generate extractive summaries\n",
        "def generate_extractive_summaries(transcript_file_path, metadata_file_path, start_idx, end_idx, frame_size=400, chunk_size=150, stride_gap=-0.5,no_repeat_ngram_size=3):\n",
        "    df_transcript = pd.read_csv(transcript_file_path)\n",
        "    df_metadata = pd.read_csv(metadata_file_path, delimiter=\"\\t\")\n",
        "    processed = 0  # Initialize the number of transcripts processed\n",
        "    for idx in range(start_idx, end_idx):\n",
        "        transcript = df_transcript['Transcript'][idx]\n",
        "        id = df_transcript['ID'][idx]\n",
        "        num_frames = int(math.ceil(len(transcript) / float(frame_size)))\n",
        "        combined_summary = \"\"\n",
        "        for i in range(num_frames):\n",
        "            start = max(0, int(i * frame_size + stride_gap * chunk_size) - chunk_size//2)\n",
        "            end = min(len(transcript), int((i+1)*frame_size + stride_gap * chunk_size) + chunk_size//2)\n",
        "            chunk = transcript[start:end]\n",
        "            summary = generate_extractive_summary(chunk)\n",
        "            combined_summary += summary + \" \"\n",
        "        # Calculate Rouge score and add it to the summary dictionary\n",
        "        reference_summary = df_metadata['episode_description'][idx]\n",
        "        rouge_scores = calculate_rouge(combined_summary, reference_summary)\n",
        "        summary = {\"ID\": id, \"Extractive_Summary\": combined_summary, \"Rouge\": rouge_scores}\n",
        "        # Write each summary and ID to individual JSON files\n",
        "        with open('/content/gdrive/MyDrive/Spotify/subsets/Pegasus_Extractive_L1/test/'f\"summary_{id}.json\", 'w') as f:\n",
        "            json.dump(summary, f)\n",
        "        processed += 1  # Increment the number of transcripts processed\n",
        "        print(f\"Processed {processed} out of {end_idx-start_idx} transcripts\")  # Print the progress update\n",
        "    print(f\"Generated and saved extractive summaries for transcripts with IDs between {start_idx} and {end_idx}\")\n",
        "\n",
        "# Define start and end indices for the transcripts to summarize\n",
        "start_idx = 0\n",
        "end_idx=1\n",
        "\n",
        "transcript_file_path='/content/gdrive/MyDrive/Spotify/subsets/500/test.csv'\n",
        "metadata_file_path='/content/gdrive/MyDrive/Spotify/subsets/500/metadata.tsv'\n",
        "\n",
        "generate_extractive_summaries(transcript_file_path, \n",
        "                              metadata_file_path, \n",
        "                              start_idx, \n",
        "                              end_idx, \n",
        "                              frame_size=500, \n",
        "                              chunk_size=250, \n",
        "                              stride_gap=-0.75, \n",
        "                              no_repeat_ngram_size=3)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVeNMCwUgvl6"
      },
      "outputs": [],
      "source": [
        "# Load the metadata file\n",
        "metadata_file_path = '/content/gdrive/MyDrive/Spotify/subsets/500/metadata.tsv'\n",
        "df_metadata = pd.read_csv(metadata_file_path, delimiter='\\t')\n",
        "\n",
        "# Create an empty dataframe to store the IDs and descriptions\n",
        "df_id = pd.DataFrame(columns=['id', 'episode_description'])\n",
        "\n",
        "# Iterate over each row in the metadata file\n",
        "for index, row in df_metadata.iterrows():\n",
        "    episode_id = row['show_uri'].split(':')[2].lower()\n",
        "    print(episode_id)\n",
        "    # Get the corresponding episode_description\n",
        "    episode_description = row['episode_description']\n",
        "    print(episode_description)\n",
        "    # Append the ID and episode_description to the df_id dataframe\n",
        "    df_id = df_id.append({'id': episode_id, 'episode_description': episode_description}, ignore_index=True)\n",
        "\n",
        "output_file_path = \"/content/gdrive/MyDrive/Spotify/subsets/Pegasus_Extractive_L1/ids_metadata.tsv\"\n",
        "df_id.to_csv(\"/content/gdrive/MyDrive/Spotify/subsets/Pegasus_Extractive_L1/ids_metadata.tsv\", sep='\\t', index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymR-EBUKwzYA",
        "outputId": "fb5a54bc-1bc1-4d13-9f48-def709cdc291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 105360 entries, 0 to 105359\n",
            "Data columns (total 2 columns):\n",
            " #   Column               Non-Null Count   Dtype \n",
            "---  ------               --------------   ----- \n",
            " 0   id                   105360 non-null  object\n",
            " 1   episode_description  105155 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.6+ MB\n",
            "None\n",
            "No metadata found for ID 7nemplvhbs0suilrkkl3ia.\n"
          ]
        }
      ],
      "source": [
        "metadata_file_path_id = '/content/gdrive/MyDrive/Spotify/subsets/Pegasus_Extractive_L1/ids_metadata.tsv'\n",
        "df_metadata_id = pd.read_csv(metadata_file_path_id, delimiter=\"\\t\")\n",
        "print(df_metadata_id.info())\n",
        "\n",
        "\n",
        "id = \"7nemplvhbs0suilrkkl3ia\"  # example ID\n",
        "\n",
        "if id in df_metadata_id[\"id\"].values:\n",
        "    print(f\"Metadata found for ID {id}.\")\n",
        "else:\n",
        "    print(f\"No metadata found for ID {id}.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwQJrCuIPaiC"
      },
      "outputs": [],
      "source": [
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, pipeline\n",
        "import math\n",
        "from rouge import Rouge\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the Pegasus models and tokenizer for extractive summarization\n",
        "tokenizer_ext = PegasusTokenizer.from_pretrained('google/pegasus-large', max_length=1024, device='cuda')\n",
        "model_ext = PegasusForConditionalGeneration.from_pretrained('google/pegasus-large', max_length=1024)\n",
        "\n",
        "# Define a function to generate an extractive summary for a given chunk of text\n",
        "def generate_extractive_summary(transcript):\n",
        "    summarizer = pipeline('summarization', model=model_ext, tokenizer=tokenizer_ext, device=0)\n",
        "    max_chunk_size = 8192\n",
        "    chunks = [transcript[i:i+max_chunk_size] for i in range(0, len(transcript), max_chunk_size)]\n",
        "    summary = ''\n",
        "    for chunk in chunks:\n",
        "        summary += summarizer(chunk, max_length=150, min_length=40, do_sample=False, no_repeat_ngram_size=3)[0]['summary_text']\n",
        "    return summary\n",
        "\n",
        "# Define a function to calculate the Rouge score for a given summary and reference summary\n",
        "def calculate_rouge(summary, reference_summary):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(summary, reference_summary)[0]\n",
        "    return scores\n",
        "\n",
        "# Define a function to read the transcript from a CSV file and generate extractive summaries\n",
        "def generate_extractive_summaries(transcript_file_path, metadata_file_path, start_idx, end_idx, frame_size=400, chunk_size=150, stride_gap=-0.5, no_repeat_ngram_size=3):\n",
        "    df_transcript = pd.read_csv(transcript_file_path)\n",
        "    df_metadata = pd.read_csv(metadata_file_path, delimiter=\"\\t\")\n",
        "    processed = 0  # Initialize the number of transcripts processed\n",
        "    for idx in range(start_idx, end_idx):\n",
        "        transcript = df_transcript['Transcript'][idx]\n",
        "        id = df_transcript['ID'][idx].lower()  # Convert ID to lowercase\n",
        "        print(f\"Processing transcript ID {id}\")\n",
        "        if id in df_metadata['id'].values:\n",
        "            print(f\"ID {id} found in metadata file.\")\n",
        "            reference_summary = df_metadata.loc[df_metadata['id'] == id, 'episode_description'].iloc[0]\n",
        "        else:\n",
        "            print(f\"ID {id} not found in metadata file. Moving on to the next transcript.\")\n",
        "            continue\n",
        "        num_frames = int(math.ceil(len(transcript) / float(frame_size)))\n",
        "        combined_summary = \"\"\n",
        "        for i in range(num_frames):\n",
        "            start = max(0, int(i * frame_size + stride_gap * chunk_size) - chunk_size//2)\n",
        "            end = min(len(transcript), int((i+1)*frame_size + stride_gap * chunk_size) + chunk_size//2)\n",
        "            chunk = transcript[start:end]\n",
        "            print(\"Chunk\", chunk)\n",
        "            summary = generate_extractive_summary(chunk)\n",
        "            print(\"Summary\",summary)\n",
        "            combined_summary += summary + \" \"\n",
        "        rouge_scores = calculate_rouge(combined_summary, reference_summary)\n",
        "        summary = {\"ID\": id, \"Extractive_Summary\": combined_summary, \"Rouge\": rouge_scores}\n",
        "        # Write each summary and ID to individual JSON files\n",
        "        with open(f\"/content/gdrive/MyDrive/Spotify/subsets/Pegasus_Extractive_L1/summary_{id}.json\", 'w') as f:\n",
        "            json.dump(summary, f)\n",
        "        processed += 1  # Increment the number of transcripts processed\n",
        "        print(f\"Processed {processed} out of {end_idx-start_idx} transcripts\")  # Print the progress update\n",
        "    print(f\"Generated and saved extractive summaries for transcripts with IDs between {start_idx} and {end_idx}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define start and end indices for the transcripts to summarize\n",
        "start_idx = 20\n",
        "end_idx=30\n",
        "\n",
        "\n",
        "transcript_file_path = '/content/gdrive/MyDrive/Spotify/subsets/Pegasus_Extractive_L1/extract.csv'\n",
        "metadata_file_path = '/content/gdrive/MyDrive/Spotify/subsets/Pegasus_Extractive_L1/ids_metadata.tsv'\n",
        "\n",
        "\n",
        "generate_extractive_summaries(transcript_file_path, \n",
        "                              metadata_file_path, \n",
        "                              start_idx, \n",
        "                              end_idx, \n",
        "                              frame_size=250, \n",
        "                              chunk_size=150, \n",
        "                              stride_gap=-0.50, \n",
        "                              no_repeat_ngram_size=3)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWLp_kZfRmZV"
      },
      "source": [
        "# Pegasus Layer 2 - Extractive + Abstractive with Rouge scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_TCLkxLm4PG"
      },
      "outputs": [],
      "source": [
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, pipeline\n",
        "import math\n",
        "from rouge import Rouge\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load the Pegasus models and tokenizer for extractive summarization\n",
        "tokenizer_ext = PegasusTokenizer.from_pretrained('google/pegasus-large', max_length=1024, device='cuda')\n",
        "model_ext = PegasusForConditionalGeneration.from_pretrained('google/pegasus-large', max_length=512)\n",
        "\n",
        "# Load the Pegasus models and tokenizer for abstractive summarization\n",
        "tokenizer_abs = PegasusTokenizer.from_pretrained('google/pegasus-large', max_length=1024)\n",
        "model_abs = PegasusForConditionalGeneration.from_pretrained('google/pegasus-large')\n",
        "\n",
        "\n",
        "# Define a function to generate an extractive summary for a given chunk of text\n",
        "def generate_extractive_summary(transcript, model, tokenizer, extractive_chunk_size,no_repeat_ngram_size):\n",
        "    summarizer = pipeline('summarization', model=model, tokenizer=tokenizer, device=0)\n",
        "    max_chunk_size = 150\n",
        "    chunks = [transcript[i:i+max_chunk_size] for i in range(0, len(transcript), max_chunk_size)]\n",
        "    summary = ''\n",
        "    for chunk in chunks:\n",
        "        summary += summarizer(chunk, max_length=150, min_length=40, do_sample=False,no_repeat_ngram_size=3)[0]['summary_text']\n",
        "    return summary\n",
        "\n",
        "\n",
        "# Define a function to generate an abstractive summary for a given extractive summary\n",
        "def generate_abstractive_summary(summary,model,tokenizer,no_repeat_ngram_size):\n",
        "    max_chunk_size = 150\n",
        "    chunks = [summary[i:i+max_chunk_size] for i in range(0, len(summary), max_chunk_size)]\n",
        "    abstractive_summary = ''\n",
        "    for chunk in chunks:\n",
        "        input_ids = tokenizer_abs.encode(chunk, return_tensors='pt', truncation=True, max_length=1024)\n",
        "        output = model_abs.generate(input_ids, max_length=128, num_beams=4, early_stopping=True,no_repeat_ngram_size=3)\n",
        "        abstractive_summary += tokenizer_abs.decode(output[0], skip_special_tokens=True)\n",
        "    return abstractive_summary\n",
        "\n",
        "\n",
        "# Define a function to calculate the Rouge score for a given summary and reference summary\n",
        "def calculate_rouge(summary, reference_summary):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(summary, reference_summary)[0]\n",
        "    return scores\n",
        "\n",
        "def generate_summaries(transcript_file_path, metadata_file_path, start_idx, end_idx, frame_size=400, extractive_chunk_size=150, stride_gap=-0.5,no_repeat_ngram_size=3):\n",
        "\n",
        "    df_transcript = pd.read_csv(transcript_file_path)\n",
        "    df_metadata = pd.read_csv(metadata_file_path, delimiter=\"\\t\")\n",
        "    processed = 0  # Initialize the number of transcripts processed\n",
        "    for idx in range(start_idx, end_idx):\n",
        "        transcript = df_transcript['Transcript'][idx]\n",
        "        num_frames = int(math.ceil(len(transcript) / float(frame_size)))\n",
        "        combined_summary = \"\"\n",
        "        combined_summary_abs = \"\"\n",
        "        for i in range(num_frames):\n",
        "            start = max(0, int(i * frame_size + stride_gap * extractive_chunk_size) - extractive_chunk_size//2)\n",
        "            end = min(len(transcript), int((i+1)*frame_size + stride_gap * extractive_chunk_size) + extractive_chunk_size//2)\n",
        "            chunk = transcript[start:end]\n",
        "            summary_ext = generate_extractive_summary(chunk, model_ext, tokenizer_ext, extractive_chunk_size,no_repeat_ngram_size)\n",
        "            summary_abs = generate_abstractive_summary(summary_ext, model_abs, tokenizer_abs,no_repeat_ngram_size)\n",
        "            combined_summary += summary_ext + \" \"\n",
        "            combined_summary_abs += summary_abs + \" \"\n",
        "        combined_summary = combined_summary.strip()  # Remove trailing whitespace\n",
        "        combined_summary_abs = combined_summary_abs.strip()  # Remove trailing whitespace\n",
        "        # Calculate Rouge scores and add them to the summary dictionary\n",
        "        reference_summary = df_metadata['episode_description'][idx]\n",
        "        rouge_scores_ext = calculate_rouge(combined_summary, reference_summary)\n",
        "        rouge_scores_abs = calculate_rouge(combined_summary_abs, reference_summary)\n",
        "        summary = {\"ID\": id, \"Combined_Summary_Ext\": combined_summary, \"Combined_Summary_Abs\": combined_summary_abs, \"Rouge_Ext\": rouge_scores_ext, \"Rouge_Abs\": rouge_scores_abs}\n",
        "        # Write each summary and ID to individual JSON files\n",
        "        with open('/content/gdrive/MyDrive/Spotify/subsets/500/summaries_train_pegasus_stride_05_abstractive/'f\"summary_{id}.json\", 'w') as f:\n",
        "            json.dump(summary, f)\n",
        "        processed += 1  # Increment the number of transcripts processed\n",
        "        print(f\"Processed {processed} out of {end_idx-start_idx} transcripts\")  # Print the progress update\n",
        "    print(f\"Generated and saved summaries for transcripts with IDs between {start_idx} and {end_idx}\")\n",
        "\n",
        "\n",
        "    # Define start and end indices for the transcripts to summarize\n",
        "start_idx = 0\n",
        "end_idx = 2\n",
        "\n",
        "# Define the file paths for the transcript and metadata files\n",
        "transcript_file_path = '/content/gdrive/MyDrive/Spotify/subsets/500/train.csv'\n",
        "metadata_file_path = '/content/gdrive/MyDrive/Spotify/subsets/500/metadata.tsv'\n",
        "\n",
        "generate_summaries(transcript_file_path, metadata_file_path, start_idx, end_idx, frame_size=400, extractive_chunk_size=150, stride_gap=-0.5,no_repeat_ngram_size=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg6pYQVKH-K_"
      },
      "outputs": [],
      "source": [
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, pipeline\n",
        "import math\n",
        "from rouge import Rouge\n",
        "import pandas as pd\n",
        "import json\n",
        "# Load the Pegasus models and tokenizer for extractive summarization\n",
        "tokenizer_ext = PegasusTokenizer.from_pretrained('google/pegasus-large',  max_length=1024,device='cuda')\n",
        "model_ext = PegasusForConditionalGeneration.from_pretrained('google/pegasus-large', max_length=1024)\n",
        "# Load the Pegasus models and tokenizer for abstractive summarization\n",
        "tokenizer_abs = PegasusTokenizer.from_pretrained('google/pegasus-cnn_dailymail',  max_length=1024,device='cuda')\n",
        "model_abs = PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail', max_length=1024)\n",
        "# Define a function to generate an extractive summary for a given chunk of text\n",
        "def generate_extractive_summary(transcript):\n",
        "    summarizer = pipeline('summarization', model=model_ext, tokenizer=tokenizer_ext, device=0)\n",
        "    max_chunk_size = 8192\n",
        "    chunks = [transcript[i:i+max_chunk_size] for i in range(0, len(transcript), max_chunk_size)]\n",
        "    summary = ''\n",
        "    for chunk in chunks:\n",
        "        summary += summarizer(chunk, max_length=150, min_length=40, do_sample=False, no_repeat_ngram_size=3)[0]['summary_text']\n",
        "    return summary\n",
        "def generate_abstractive_summary(text, model, tokenizer, no_repeat_ngram_size, max_length=200):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
        "    # Generate the abstractive summary\n",
        "    summary_ids = model.generate(inputs,\n",
        "                                 num_beams=4,\n",
        "                                 length_penalty=0.8,\n",
        "                                 min_length=75,\n",
        "                                 max_length=200,\n",
        "                                 no_repeat_ngram_size=2,\n",
        "                                 early_stopping=False)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "# Define a function to generate both extractive and abstractive summaries and compute the ROUGE scores\n",
        "def generate_summaries(transcript_file_path, metadata_file_path, start_idx, stop_idx, model_abs, tokenizer_abs, model_ext, tokenizer_ext, frame_size=400, extractive_chunk_size=150, stride_gap=-0.5, no_repeat_ngram_size=3, abstractive_min_length=100, max_length=200):\n",
        "    df_transcript = pd.read_csv(transcript_file_path)\n",
        "    df_metadata = pd.read_csv(metadata_file_path, delimiter=\"\\t\")\n",
        "    rouge = Rouge()\n",
        "    rouge_scores = []\n",
        "    processed = 0  # Initialize the number of transcripts processed\n",
        "    for idx in range(start_idx, stop_idx):\n",
        "        transcript = df_transcript['Transcript'][idx]\n",
        "        print(f\"The Original transcript ({len(transcript.split())} words): {transcript}\")\n",
        "        id = df_transcript['ID'][idx]\n",
        "        num_frames = int(math.ceil(len(transcript) / float(frame_size)))\n",
        "        print(\"################################\")\n",
        "        print(f\"No of frames for Extractive Summary: {num_frames}\")\n",
        "        print(f\"Transcript ID: {id}\")\n",
        "        summary_ext_chunks = []\n",
        "        summary_abs_chunks = []\n",
        "        for i in range(num_frames):\n",
        "            start = max(0, int(i * frame_size + stride_gap * extractive_chunk_size) - extractive_chunk_size//2)\n",
        "            end = min(len(transcript), int((i+1)*frame_size + stride_gap * extractive_chunk_size) + extractive_chunk_size//2)\n",
        "            chunk = transcript[start:end]\n",
        "            #print(f\"Chunk {i+1} before extractive summary generation: {chunk}\")\n",
        "            summary_ext = generate_extractive_summary(chunk)\n",
        "            summary_ext_chunks.append(summary_ext)\n",
        "            #print(f\"Extractive summary for chunk {i+1}: {summary_ext}\")\n",
        "        summary_ext = ' '.join(summary_ext_chunks)\n",
        "        print(f\"Final Extractive summary ({len(summary_ext.split())} words): {summary_ext}\")\n",
        "        combined_summary_abs_chunks = []\n",
        "        num_frames_abs = int(math.ceil(len(summary_ext) / float(frame_size)))\n",
        "        print(\"#################################\")\n",
        "        print(f\"No of frames for Abstractive Summary: {num_frames_abs}\")\n",
        "        for i in range(num_frames_abs):\n",
        "            start = max(0, int(i * frame_size + stride_gap * extractive_chunk_size) - extractive_chunk_size//2)\n",
        "            end = min(len(summary_ext), int((i+1)*frame_size + stride_gap * extractive_chunk_size) + extractive_chunk_size//2)\n",
        "            chunk = summary_ext[start:end]\n",
        "            #print(f\"Chunk {i+1} before abstractive summary generation: {chunk}\")\n",
        "            summary_abs = generate_abstractive_summary(chunk, model_abs, tokenizer_abs, no_repeat_ngram_size, max_length=max_length)\n",
        "            combined_summary_abs_chunks.append(summary_abs)\n",
        "            #print(f\"Abstractive Summary for Chunk {i+1} : {summary_abs}\")\n",
        "        combined_summary_abs = ' '.join(combined_summary_abs_chunks)\n",
        "        print(f\"Final Abstractive summary  {combined_summary_abs}\")\n",
        "        # If the summary is too long, recursively generate a shorter summary\n",
        "        if len(combined_summary_abs.split()) > abstractive_min_length:\n",
        "            print(f\"Abstractive summary before 100 words: {combined_summary_abs}\")\n",
        "            combined_summary_abs = combined_summary_abs.strip()  # Remove trailing space\n",
        "            #print(f\"\\nCombined abstractive summary for transcript ID {id}: {combined_summary_abs}\")\n",
        "            # If the summary is too long, recursively generate a shorter summary\n",
        "            while len(combined_summary_abs.split()) > abstractive_min_length:\n",
        "              #print(f\"Abstractive summary before {abstractive_min_length} words: {combined_summary_abs}\")\n",
        "              combined_summary_abs = generate_abstractive_summary(combined_summary_abs, model_abs, tokenizer_abs, no_repeat_ngram_size, max_length=max_length-10)\n",
        "              #print(f\"Abstractive summary after {abstractive_min_length} words: {combined_summary_abs}\")\n",
        "              combined_summary_abs = combined_summary_abs.strip()  # Remove trailing space\n",
        "        print(f\"\\nFinal combined abstractive summary for transcript ID {id} ({len(combined_summary_abs.split())} words):: {combined_summary_abs}\")\n",
        "        # Compute the ROUGE scores for the extractive and abstractive summaries\n",
        "        reference_summary = df_metadata['episode_description'][idx]\n",
        "        summary_ext = summary_ext.replace('\\n', ' ')\n",
        "        summary_abs = summary_abs.replace('\\n', ' ')\n",
        "        combined_summary_abs = combined_summary_abs.replace('\\n', ' ')\n",
        "        rouge_scores.append({\n",
        "            'ID': id,\n",
        "            'Abstractive Summary': combined_summary_abs,\n",
        "            'Abstractive ROUGE-1': rouge.get_scores(combined_summary_abs, reference_summary)[-1]['rouge-1']['f'],\n",
        "            'Abstractive ROUGE-2': rouge.get_scores(combined_summary_abs, reference_summary)[-1]['rouge-2']['f'],\n",
        "            'Abstractive ROUGE-L': rouge.get_scores(combined_summary_abs, reference_summary)[-1]['rouge-l']['f']\n",
        "        })\n",
        "        # Save the ROUGE scores to a JSON file for the current transcript ID\n",
        "        with open('/content/gdrive/MyDrive/Spotify/subsets/Pegasus_Extractive_Abstractive_L2/'f\"summary_{id}.json\", 'w') as f:\n",
        "            json.dump(rouge_scores, f)\n",
        "          # Clear the rouge_scores list for the next transcript ID\n",
        "        rouge_scores = []\n",
        "\n",
        "\n",
        "# Define the file paths for the transcript and metadata files\n",
        "transcript_file_path = '/content/gdrive/MyDrive/Spotify/subsets/Pegasus_Extractive_L1/valid.csv'\n",
        "metadata_file_path = '/content/gdrive/MyDrive/Spotify/subsets/Pegasus_Extractive_L1/ids_metadata.tsv'\n",
        "#target_id = '0wEYi9uEvNOetrxrIJ4wJq'\n",
        "\n",
        "\n",
        "start_idx = 31\n",
        "stop_idx = 41\n",
        "\n",
        "# Call the generate_summaries method\n",
        "generate_summaries(transcript_file_path, metadata_file_path, start_idx, stop_idx, model_abs, tokenizer_abs, model_ext, tokenizer_ext, frame_size=400, extractive_chunk_size=150, stride_gap=-0.5, no_repeat_ngram_size=3, abstractive_min_length=100, max_length=200)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bgPj7nJspXM",
        "outputId": "09ba5e66-d291-483f-8693-c2df90ba9864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average ROUGE-1 score: 0.0761\n",
            "Average ROUGE-2 score: 0.0083\n",
            "Average ROUGE-L score: 0.0705\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Set the path to the directory containing the JSON files\n",
        "dir_path = '/content/gdrive/MyDrive/Spotify/subsets/500/summaries_train_pegasus_stride_05'\n",
        "\n",
        "# Initialize a dictionary to store the ROUGE scores for each metric\n",
        "rouge_scores = {'rouge-1': [], 'rouge-2': [], 'rouge-l': []}\n",
        "\n",
        "# Iterate over the files in the directory\n",
        "for filename in os.listdir(dir_path):\n",
        "    if filename.endswith('.json'):\n",
        "        # Load the JSON file\n",
        "        with open(os.path.join(dir_path, filename), 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Extract the ROUGE scores for each metric\n",
        "        rouge_1_score = data['Rouge']['rouge-1']['f']\n",
        "        rouge_2_score = data['Rouge']['rouge-2']['f']\n",
        "        rouge_l_score = data['Rouge']['rouge-l']['f']\n",
        "\n",
        "        # Append the ROUGE scores to the appropriate list in the dictionary\n",
        "        rouge_scores['rouge-1'].append(rouge_1_score)\n",
        "        rouge_scores['rouge-2'].append(rouge_2_score)\n",
        "        rouge_scores['rouge-l'].append(rouge_l_score)\n",
        "\n",
        "# Calculate the average ROUGE score for each metric\n",
        "avg_rouge_1_score = sum(rouge_scores['rouge-1']) / len(rouge_scores['rouge-1'])\n",
        "avg_rouge_2_score = sum(rouge_scores['rouge-2']) / len(rouge_scores['rouge-2'])\n",
        "avg_rouge_l_score = sum(rouge_scores['rouge-l']) / len(rouge_scores['rouge-l'])\n",
        "\n",
        "# Print the average ROUGE scores for each metric\n",
        "print(\"Average ROUGE-1 score: {:.4f}\".format(avg_rouge_1_score))\n",
        "print(\"Average ROUGE-2 score: {:.4f}\".format(avg_rouge_2_score))\n",
        "print(\"Average ROUGE-L score: {:.4f}\".format(avg_rouge_l_score))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARq1A2boAqsN",
        "outputId": "95d3360f-7cc6-447a-ba26-b4e5e000b5d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of JSON files:  374\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set the path to the directory containing the JSON files\n",
        "dir_path = '/content/gdrive/MyDrive/Spotify/subsets/500/summaries_train_pegasus_stride_05'\n",
        "\n",
        "# Initialize a counter variable\n",
        "count = 0\n",
        "\n",
        "# Iterate over the files in the directory\n",
        "for filename in os.listdir(dir_path):\n",
        "    if filename.endswith('.json'):\n",
        "        # Increment the counter if the file is a JSON file\n",
        "        count += 1\n",
        "\n",
        "# Print the number of JSON files\n",
        "print(\"Number of JSON files: \", count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_AVklqWIrXU",
        "outputId": "7dcdf09c-3dbd-46f9-8d05-ca4527ebcfea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows in the CSV file:  401\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# Set the path to the CSV file\n",
        "csv_path = '/content/gdrive/MyDrive/Spotify/subsets/500/train.csv'\n",
        "\n",
        "# Initialize a counter variable\n",
        "count = 0\n",
        "\n",
        "# Open the CSV file with 'utf-8' encoding\n",
        "with open(csv_path, 'r', encoding='utf-8') as csv_file:\n",
        "    # Create a CSV reader object\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "\n",
        "    # Iterate over the rows in the CSV file\n",
        "    for row in csv_reader:\n",
        "        # Increment the counter for each row\n",
        "        count += 1\n",
        "\n",
        "# Print the number of rows in the CSV file\n",
        "print(\"Number of rows in the CSV file: \", count)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvxK-OoobWFx",
        "outputId": "c1da8303-7755-4571-ba75-f0fc47cf13e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['show_uri', 'show_name', 'show_description', 'publisher', 'language',\n",
            "       'rss_link', 'episode_uri', 'episode_name', 'episode_description',\n",
            "       'duration', 'show_filename_prefix', 'episode_filename_prefix'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "metadata_file_path = '/content/gdrive/MyDrive/Spotify/subsets/500/metadata.tsv'\n",
        "df_metadata = pd.read_csv(metadata_file_path, delimiter=\"\\t\")\n",
        "print(df_metadata.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiUC1KqVP5F2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import pandas as pd\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "from rouge import Rouge\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the Pegasus models and tokenizer for abstractive summarization\n",
        "model_abs = PegasusForConditionalGeneration.from_pretrained('google/pegasus-large')\n",
        "tokenizer_abs = PegasusTokenizer.from_pretrained('google/pegasus-large')\n",
        "\n",
        "# Define a function to generate an abstractive summary for a given chunk of text\n",
        "def generate_abstractive_summary(text):\n",
        "    inputs = tokenizer_abs.encode(text, return_tensors='pt', truncation=True, max_length=1024)\n",
        "    summary_ids = model_abs.generate(inputs, num_beams=4, length_penalty=2.0, max_length=256, early_stopping=True)\n",
        "    summary = tokenizer_abs.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Define a function to calculate the Rouge score for a given summary and reference summary\n",
        "def calculate_rouge(summary, reference_summary):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(summary, reference_summary)[0]\n",
        "    return scores\n",
        "\n",
        "# Define a function to read the extractive summaries from JSON files, generate abstractive summaries and write the results to JSON files\n",
        "def generate_abstractive_summaries(json_file_path, metadata_file_path, chunk_size, frame_size, stride_overlap):\n",
        "    df_metadata = pd.read_csv(metadata_file_path, delimiter=\"\\t\")\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        id = data['ID']\n",
        "        extractive_summary = data['Extractive_Summary']\n",
        "        sentences = nltk.sent_tokenize(extractive_summary)\n",
        "        abstractive_summary = \"\"\n",
        "        for i in range(0, len(sentences), int(len(sentences)*stride_overlap)):\n",
        "            frame = sentences[i:i+frame_size]\n",
        "            chunks = [frame[j:j+chunk_size] for j in range(0, len(frame), chunk_size)]\n",
        "            for chunk in chunks:\n",
        "                text = \" \".join(chunk)\n",
        "                summary = generate_abstractive_summary(text)\n",
        "                abstractive_summary += summary.strip() + \" \"\n",
        "        abstractive_summary = abstractive_summary.strip()\n",
        "        reference_summary = df_metadata[df_metadata['ID'] == id]['episode_description'].values[0]\n",
        "        rouge_scores = calculate_rouge(abstractive_summary, reference_summary)\n",
        "        summary = {\"ID\": id, \"Extractive_Summary\": extractive_summary, \"Abstractive_Summary\": abstractive_summary, \"Rouge\": rouge_scores}\n",
        "        with open('/content/gdrive/MyDrive/Spotify/subsets/500/summaries_train_pegasus_stride_05_abstractive/'f\"summary_{id}.json\", 'w') as f:\n",
        "            json.dump(summary, f)\n",
        "        print(f\"Generated abstractive summary for transcript ID {id}\")\n",
        "\n",
        "\n",
        "\n",
        "# Define the file paths and IDs of the transcripts to summarize\n",
        "json_file_path = '/content/gdrive/MyDrive/Spotify/subsets/500/summaries_train_pegasus_stride_05/'\n",
        "metadata_file_path = '/content/gdrive/MyDrive/Spotify/subsets/500/metadata.tsv'\n",
        "\n",
        "# Specify the number of JSON files to process at a time\n",
        "batch_size = 5\n",
        "\n",
        "# Get a list of all JSON files in the specified directory\n",
        "json_files = [os.path.join(json_file_path, f) for f in os.listdir(json_file_path) if f.endswith('.json')]\n",
        "\n",
        "\n",
        "# Loop through all JSON files and generate abstractive summaries for each batch\n",
        "for i in range(0, len(json_files), batch_size):\n",
        "    batch_files = json_files[i:i+batch_size]\n",
        "    for json_file in batch_files:\n",
        "        generate_abstractive_summaries(json_file, metadata_file_path, 200, 400, -0.5)\n",
        "\n",
        "\n",
        "df_metadata = pd.read_csv(metadata_file_path, delimiter=\"\\t\")\n",
        "print(df_metadata.columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsfpA9LG9kom"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def extract_rows_with_ids(ids, input_file_path, output_file_path, directory_path):\n",
        "    \"\"\"\n",
        "    Extracts the rows with the given transcript IDs from the input file and writes them to a new CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    ids (list): A list of transcript IDs to extract.\n",
        "    input_file_path (str): The path to the input file.\n",
        "    output_file_path (str): The path to the output file.\n",
        "    directory_path (str): The path to the directory containing the files with the transcript IDs as suffixes.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # check if each ID in the list has a corresponding file in the directory\n",
        "    for id in ids:\n",
        "        file_path = os.path.join(directory_path, f\"{id}.csv\")\n",
        "        if not os.path.isfile(file_path):\n",
        "            raise ValueError(f\"No file found for ID {id} in directory {directory_path}\")\n",
        "    \n",
        "    # read the csv file into a pandas dataframe\n",
        "    df = pd.read_csv(input_file_path)\n",
        "    \n",
        "    # lowercase the IDs in the list\n",
        "    ids = [id.lower() for id in ids]\n",
        "    \n",
        "    # lowercase the IDs in the 'ID' column of the dataframe\n",
        "    df['ID'] = df['ID'].str.lower()\n",
        "    \n",
        "    # filter the dataframe to include only the rows with the given IDs\n",
        "    filtered_df = df[df['ID'].isin(ids)]\n",
        "    \n",
        "    # write the filtered dataframe to a new CSV file\n",
        "    filtered_df.to_csv(output_file_path, index=False)\n",
        "    \n",
        "    # print a message indicating the output file path\n",
        "    print(f\"Extracted rows with IDs {ids} and saved to {output_file_path}\")\n",
        "\n",
        "# Example usage:\n",
        "ids_to_extract = [\n",
        "    \n",
        "    ]\n",
        "input_file_path = '/content/gdrive/MyDrive/Spotify/subsets/500/train.csv'\n",
        "output_file_path = '/content/gdrive/MyDrive/Spotify/subsets/Pegasus_Extractive_L1/extract.csv'\n",
        "directory_path = '/content/gdrive/MyDrive/Spotify/subsets/Model_Output/Pegasus_Extractive_Abstractive_L2/'\n",
        "\n",
        "# extract the rows with the given IDs and write the resulting dataframe to a CSV file\n",
        "extract_rows_with_ids(ids_to_extract, input_file_path, output_file_path, directory_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the directory path\n",
        "dir_path = '/content/gdrive/MyDrive/Spotify/subsets/Model_Output/Pegasus_Extractive_Abstractive_L2/'\n",
        "\n",
        "# Loop through all files in the directory\n",
        "for filename in os.listdir(dir_path):\n",
        "    \n",
        "    # Check if the file is in the correct format\n",
        "    if filename.endswith('.json') and filename.startswith('summary_'):\n",
        "        \n",
        "        # Get the ID from the filename\n",
        "        id = filename[len('summary_'):-len('.json')]\n",
        "        \n",
        "        # Print the ID\n",
        "        print(id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTYojSgwm9pB",
        "outputId": "4e7c2dd0-66b1-4ec2-afb2-4499d60fe17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7CcAQwKpwz7W7Tybbj97Km\n",
            "57bfPuB7VYdpv55KsWhEbT\n",
            "6YeWUBptv0OHarljT8Wwrf\n",
            "5ZApaov7Xx7ntTVlfpdAxY\n",
            "28CAQkt2GblBPK4vo4aE32\n",
            "5gq7aguWdeGnJfW55oburg\n",
            "7oUo2HJANZlErbhC5y3xN1\n",
            "7dcD8SwiaikzogvWbdCuPM\n",
            "58rD5mU99LDGYeNxaPTOOF\n",
            "2ZNfMu94hWwMIaE5xf0DMr\n",
            "5GDrhuy2K2Om2l3KyDjutK\n",
            "3jte3EHLegwz7QrmQQfd7L\n",
            "0uzsB2RHbmWxICsNi5DyXd\n",
            "3w26WUIXyCQ1suqaKbWjuo\n",
            "6lAC1hLD7uODrjBp8FRmjw\n",
            "28X7VZX6kIGj9PNMLto7EK\n",
            "3P0sUNoADKL9HPolfIgTK5\n",
            "7vq9Fid8DS6NMUQwdpAXZg\n",
            "6e6Tqpy9gQehm3BsR7h5bg\n",
            "1eYf1f2rACWiIIBVj4v77w\n",
            "3J1dmb2HsnL7nctr1wpjX5\n",
            "3jMe7zTRTwCm7q7VRqjDoX\n",
            "2eAmE2HF4GEi5yXpgKkWng\n",
            "6KnWFL81JMaovhI4CEt8gH\n",
            "7nEMPlVhBS0SUilrkKl3ia\n",
            "0bLEUbqPdsFOEPck4G6FGM\n",
            "7yYi8cywoIxckyBnCRCKDN\n",
            "5NpNNtR0gx0tO5CCEEbOCg\n",
            "5eJPzdD8uU8yPpBHqQjPYS\n",
            "5yRyiAxHPGLChBqTfT3FXp\n",
            "4ISXhcr9fwxRcfCEDR2wiH\n",
            "6kqFQCO7wXieZUopdVsf9c\n",
            "4o5pSHhPy6n7NorTb9kSVu\n",
            "11KOktqHfuReK6qINXyE6r\n",
            "6kKfjQbg8ueAIQyIUkcoBQ\n",
            "5xYnrq3zbJhNgNQQYZ1cZk\n",
            "5LFtjocUcricEq9tJ5O5WP\n",
            "0IGtnJ2dQ73cZgvOkJS7HM\n",
            "6kZoeBWA7m5XhLjmsJGAZx\n",
            "1fgi6DTfcBv13S9ZVizlmz\n",
            "5O9Wetc78TGfPHcVCq8RnY\n",
            "26VtfRMBZw8feaMlDW9KDP\n",
            "1TNxTloyo4qMgcpQVKBE2A\n",
            "2riWgHX0yjg5LJo78UaDq8\n",
            "55VfynKK09foFInjoBjldC\n",
            "7tvC52DtDw5t6xiKMeedAp\n",
            "0JPSWPSvK3OaKYDuoZlVIV\n",
            "2OboA1kH7EvNrWzehAL07j\n",
            "6pCopI6BPEWTLRs3EnM5cH\n",
            "5tPnaHMwN6IGagK5hylDkJ\n",
            "73NiuDGsUGdUO7TzqOlogy\n",
            "2h9FHPhKMryN659fJN2MoO\n",
            "4BQCo2doLb2XPFg8Vvw2wj\n",
            "6zfW4UXOk27WPben1W9plZ\n",
            "2QdrCR3UWzr6XAcxuRmYnG\n",
            "5ptpXqYYvSw3TItmyzDZB6\n",
            "5S6EGc8EVYbY62gO05VcdU\n",
            "27VAh3uKAcZjsFpOA1Jmhb\n",
            "5m12r3xBe5M7C9LLodSFXd\n",
            "2VtqDmIEcehDMqwRuliNlb\n",
            "6Lha96hzn12Jt0uBFjncdD\n",
            "3sklnZCnjRFQVy7hyacacK\n",
            "1qKc0Q6usxBcCePLDEWfS6\n",
            "1RnxLLgwLkEHXSS1t3r2jn\n",
            "4c9DcH4SMFpkBX4EUGCRwM\n",
            "32tNQkYcpgFOmWOrqpvPnp\n",
            "5dQQ3gIjdBIkglUVBbLe0H\n",
            "0p1qVTqA3CoPkAaMfVz6T8\n",
            "1sYSra5nkgTMtnWuTq3yDA\n",
            "7AUiSbcR4Rp8RgJvAS0I8i\n",
            "1YPkN4R6a9KroDTQhOFdsC\n",
            "2ySBEMBb6rUFti7gaOZDcK\n",
            "0wEYi9uEvNOetrxrIJ4wJq\n",
            "3ZcwIgjwXrSbZtGJ2hnVLq\n",
            "6cLoopi1gicNNki5Pzm4qk\n",
            "3ZXWygw71BQZxOw5Zpy49o\n",
            "1I1wdGcyyS89yWuKLI6Xqx\n",
            "2dJAlamlqTj0q36iwaDS3f\n",
            "26AnQhiJYQxhZqyqcNFHBn\n",
            "6PZ7vbL79kc0Z6iznFAmu2\n",
            "05IkaHKSOiBTp4gboWltDg\n",
            "6g3dxaEdfXaIwFrHBD5Zbq\n",
            "0EpgpJCLJmNneFCJOgbH53\n",
            "7ChwFKAsip9h8FawuOInX3\n",
            "4emyVVFr7V6zKw3QJXtkHX\n",
            "4ArmZcMHZq0F1Go12mJFUr\n",
            "5COegcxRJA4XKQ9dSvfiqw\n",
            "7Jrkc09gnXex1gyiyVCyVd\n",
            "6f7HsmMKTSYxmqQ82swgiX\n",
            "2WrIDU2yDSb4GpNL66IRvw\n",
            "6LhRM2eU2GiqkKAUWOm41i\n",
            "7Iq72L6XjQBFDZncbuO70i\n",
            "1imFzI4MRnqEF1ZWQTyLpD\n",
            "5YsC1i4obbmSzcrhrzT9Og\n",
            "21bBSd86VKoRl8dMXAXSAj\n",
            "1sgVaNSGNjgOEN0oqagudB\n",
            "7EdLqE3nFt2Ct10VfzFTEt\n",
            "5y7TBVDrsrNgPVSMnZzptO\n",
            "5vufu7qU7rBqBlzzgHFnIl\n",
            "3dptqbNx7YNQgLzCgmKhje\n",
            "7362gJh9YAKp2aCTPCdLyT\n",
            "61x9jWJ3WxtU89sj68JYd7\n",
            "0lU1zAN3G4mibba2Nd8zp1\n",
            "6uekmo6xSTYnaTubniffTb\n",
            "5tVDzTCrbwjVFPX9hFGLO1\n",
            "12PwdPaWs5sYJS0qQd2R4v\n",
            "6TAw6WiP2TvNvGJ7DhZimc\n",
            "0dFPCj61WzmRmpWgB40lx9\n",
            "6j6fXmbMeY9iYu10nmn2fL\n",
            "3oXPM2UeBnGHRjnua5igyt\n",
            "0xPM9iUp8p1y0hd3Js3ADN\n",
            "52ovoaCZ3xpBMNB9o0PKLj\n",
            "23l6nHDaI7WhKyADL3qHMB\n",
            "78RydDM3R38GPd7Hq5qeKm\n",
            "7cii5zovj2CcBftHwUsHYk\n",
            "1xeEym2JXNuu3V0ap2R28v\n",
            "6myfd6OkgQokIGDFUi0JA3\n",
            "4wnI7zgYUmaQ7Es8c2xfoB\n",
            "4sJWJOKvB7gFgFf99ro80t\n",
            "5GH4edLBWkoZYGimJwD2GK\n",
            "7sRk8o6oA02KlBJvFfJiAJ\n",
            "6GNZ9pu12DiEjef2kNnLaW\n",
            "2caBifIXVxEbfIpitd1vmw\n",
            "27YrY5hCVoY6R5om2WlvN2\n",
            "1wdJZCbnm1LLX91Cj52dRX\n",
            "0unzoTTVcsjYhZqyN5VbLW\n",
            "2Qxwr88pwnpAKDYXYwLNoU\n",
            "47cL3WJJ0dmqj9uPOvM5fU\n",
            "6IW2gMuj4rGODTm5udhRNL\n",
            "509L0cD14YFdDv2WUpA8yu\n",
            "1kgHEKZDYVWw84W9OauqPq\n",
            "15IgsnbFm8lQBoVY96FXKM\n",
            "5acKbtHQiiaHNrK3DOiBpV\n",
            "6lUoybsA7u6X0QoRcx0ybq\n",
            "4Epno26OuTTjqb7hBhH5W0\n",
            "4obvOjUN1xDmQaEenIHdp3\n",
            "60Snv1qyJCwXYmOfUrAI8U\n",
            "1hdIwMmOIQGgmzBUPzkfFg\n",
            "0man1V91iDBGQSDfJjFxS2\n",
            "7oOKjei3GzDMYsc6SCSWBv\n",
            "2PkhZTpOqBaWW3DsDw9GND\n",
            "6T0eobXHsaEjvOcB43quvT\n",
            "49M677kKbSXIIeIWDucXUn\n",
            "5z4ScffnM0e5psqgVBufLG\n",
            "4iYxlGwVO6Je9A9rR6UcYi\n",
            "4ruQZ1WQPhDr6JSBKop28b\n",
            "7peXdS1osxOL6D2hMv1AY0\n",
            "3VvY0XbbHK5ahqDURugkB9\n",
            "3CZgYUtGkVOyczo3rjwsjD\n",
            "7zUT1d73onnErAWINMD1a0\n",
            "03f6AJeJVQaTSJKmn8MHBe\n",
            "32ztwyV6buaJQikTA8YhlH\n",
            "7ACLWThuHQmDsugsCI4dIy\n",
            "553FYPZsj8AyP0tMD6XjzS\n",
            "4ht03YGiLp30Vx52U6WxB3\n",
            "1GyrSPkhb64oUvgrgS3tXQ\n",
            "1ZcZbKKFTKlcl7iu3If51y\n",
            "3Zt0N8SvMUGKAmvhukuXRR\n",
            "2J6lBRGSBAkVoIR6XX6hGJ\n",
            "0WH05kqFPZcjkwwd56PmwP\n",
            "3SuIs7rlK0z3WQgDX9NoXy\n",
            "1PfyEW1gNKSXNg3msARlnO\n",
            "39aeMiB84gdVzktqfiWVmy\n",
            "0HZDHMWuK8uCCFLKQfihpo\n",
            "77jROttG4PaEx6EZRC3fV3\n",
            "1wVSp4tMl4Mbng0RJc2wh3\n",
            "7p5iNq1qc9RikrBVwK09U9\n",
            "1aaiZgBoi5qaOMr3ZrwOep\n",
            "3pwNpjf72BUtDwb8MoMO8Y\n",
            "0df99GPTOG0kp9wqGuhb0y\n",
            "7forP0dPFxLPT2ZLoQjImL\n",
            "4qCgEa8tNI6A3gCf2oVdSl\n",
            "6WnOIQTkn4mwoxZBdWp6OV\n",
            "3HqjWURy5nkjZj0RvvcD78\n",
            "3jl7LZCQ5y6uk6wK7zgAt9\n",
            "52BGIGDLpQCiL9kdv040fU\n",
            "7j4CZPFJwVRRb08cBVonTy\n",
            "6PjqerZA54f87tH0XE3zum\n",
            "4Xrix3IjmvoLEmkm8x6kWm\n",
            "6EVekqTDCaTWPCeiKGUw7v\n",
            "6RQMqjv8peLRtWlLuOzJFI\n",
            "1Qse7A1Fly4ykPIJ4ywTSR\n",
            "7vWi1eTOhAQI4gO36Q4eGC\n",
            "5pXEhAUx8ZlGEXZ4hoHkmz\n",
            "7pyFuVkor5WGyJGJIYLr34\n",
            "2JTCRLMWywUrFXJ4NOoaVl\n",
            "6RmyYdJC01IamXX2R8WOrg\n",
            "4J3hSdSwvhv1YAV8vxPngC\n",
            "15PKhbfhltOZkFXrqGkGcj\n",
            "7KvAsHU4VCI6NFpFOwE12Z\n",
            "3VCPwEHude0F6YcX4SJ1nV\n",
            "56eEfLBEGQ4Bg3pFBgFN51\n",
            "4AAsAbCs9Wi3WPp63tOD1N\n",
            "0ZwMaNDixROvFctWisd7cf\n",
            "3XmsZXPU2QnEVnDw2EnME4\n",
            "0zaZtavU9Sh7ECLqNjxeLR\n",
            "3tb99DlOxDfzQrdtX2usSy\n",
            "1JFEAzSy5coQY5Hmgvo0Ho\n",
            "3GgM8YXMuTbpizURpl5Rs2\n",
            "0KOX0XczaFkKclOacqFdR5\n",
            "7dd8LBlkyxswGXCr4GOqKB\n",
            "7hYtMve073gOIkBKva2Hzm\n",
            "6aue0UhoHPnTBz4hVrvzvk\n",
            "79ITeeQEPwP5WOw0CnlpY5\n",
            "2P8qlukN5o2UeeGxW3RpH8\n",
            "6kcD0Y6GdGgpk0GHVI2VTN\n",
            "70oCGNEEspgktK5BzL9eep\n",
            "4NUJjFIHTU0oBP92zspHdM\n",
            "5ssfG6xwsubnoQtOoqrqYA\n",
            "7JEFlYi2kCYvC8995alAfj\n",
            "6dTq0Mkz93QSeHJtc4o3tX\n",
            "6lFJXLzX7TwjQr9SEDU1IX\n",
            "1GmvDh3XpE8ZAXQlbv8ggx\n",
            "0fclXfUNwaNVyPJlLz6U0K\n",
            "7st8DXiNzcnf4pnRYMsD13\n",
            "3xQQSysLinCUw9pUCaDWtY\n",
            "1uRvnMbiAd8nKpv0E74IWh\n",
            "3gusVbaYrEx5jd8ReZlKDD\n",
            "58It34bN5u9ruaXyhxJ4JW\n",
            "3pr9ln06siFU1v1XOuVcvO\n",
            "0ByOibwAY5xC7FcP5q3otU\n",
            "5LhT47Ox9HvOvtoRTNhRyf\n",
            "13fLEVj661Sbs4svfFzIHX\n",
            "6qk2NxCbqWaRUxnyLp0wp7\n",
            "0BI3HJG064spRCWgx4ZSSM\n",
            "65Bnpdhu3SjaPwASogM5tN\n",
            "5pEiSaJexZJfGzTjOvT8Hh\n",
            "6jCiiigbbU9Pe0ThW0mbgp\n",
            "2kawrpSytMOE5lDsET5EW4\n",
            "6qTaqFw1yBlEviJGdtGvud\n",
            "5o7BEgjnAdGUOnYvLQLNf5\n",
            "1H9Gr10IQaaEtow9f2dI4W\n",
            "3e61sVMRnfcaqV2deQplQs\n",
            "783Oc1AZIQzG4gV8PTF4eT\n",
            "0LM9rb9N4G501Z9vwgVqlC\n",
            "3VAwL2E515z5JwTXZppehB\n",
            "75o3pnUk5l6wMHH6Wsz5GO\n",
            "7JhO4se35pl1bfWalAZ4Xl\n",
            "6f1iJVj1u76lMNFcrAkKcQ\n",
            "0FzA7YcvBeyzaLbZg6XJ0w\n",
            "4r24NCV085FakwwiH4EdmW\n",
            "6Ajd9JpCzfC4rpsNr44qmm\n",
            "1gSxmGbDtHbHpCsLZqA0M5\n",
            "2ELBotzNtWrUnWCsQHy0jJ\n",
            "69BIu3I10Hv06DKw0idlkw\n",
            "6znUY2OFoIxBKk1IwU6I3t\n",
            "4kWl5lABNDD8Wiezvt3aYT\n",
            "7yrpRCDrTqSBQCfpjnFCwC\n",
            "2KgM1Zsz1FYjIcMZXMATEn\n",
            "56SysBZLFJRJE5phi2fRJX\n",
            "1p5ViqBF4RNTAiqesxalov\n",
            "3oP2XuQt6uixilDeZdkVAt\n",
            "72wIZ9ZZGZZ68kteZDQ6Jl\n",
            "3h6n8Ox1225Ev7HSe8mGuJ\n",
            "56pKLlVYJn2nDhXHuNgBod\n",
            "3TFVuuNciQZxclrZHdCe4E\n",
            "6FZ1V35w6aaRmKuzatN2Cb\n",
            "416sbruGyITqWQkWvlQYM0\n",
            "0B0sB0GFsdT0LgZ6ePAWq4\n",
            "5pEYMffZYkcL9T8DMjlSsi\n",
            "71HSrIQoGJ2sKOn2ICHC5e\n",
            "0L7N070XuucttXVMDQLyKQ\n",
            "73u7flwhquz2ML7o02orVT\n",
            "2VsjfNPqqOijJ7SmdkRIwL\n",
            "2vjHrn4uJMjBFTIH5Ij3RQ\n",
            "6bc9r0rlv4WsPVUcaIi59l\n",
            "7qDI140SzXvk8wjWhqZdlR\n",
            "0jFfIBmNBDhRDGhxpxoJ2V\n",
            "6vvOeazyxvVXghlKpECSnM\n",
            "3tEwg95XkEoOHwup0Q4R9N\n",
            "4S9SzQYYkGlETDHPgFHvkx\n",
            "5dHTXyvM2SMM19OkJ4Pyru\n",
            "2XmyLiX0hFpBdGzYXOtpcQ\n",
            "4mNBWjtw0beDqFh6J2ZMJT\n",
            "0sTWge4gkqigU3iIYDY9sP\n",
            "1lbRtlz7Ycob8OBtLshwkM\n",
            "3YYaX1vcvEBKos4tXSF3ht\n",
            "6rpcDcBNFms7sTE565KZUI\n",
            "1qdWhEcLxTq7TZVQYdUOgm\n",
            "5saOx6Mz8oWqvVmgZrz3ue\n",
            "2K9vOu2kZVj00jpqzDypVg\n",
            "1Cs5I1wqfqkyXAVluswmDI\n",
            "4G81Xg7If2oKsXGItBdauk\n",
            "0oXnjYIMRzvC8cQFVqmkjf\n",
            "08Z9AaUdqHQ5EoGX9uCHBw\n",
            "1TUtsqYZEYX9TfKIIrsTSH\n",
            "7FH2bwIMEXN3bJQUfXu1RQ\n",
            "0ZolA8TZOLZwqF8DwzsX2y\n",
            "7gdFYMbiWly0NHWlm5Qrjv\n",
            "0CtBFZCn9faEDoxcRGvBMU\n",
            "0z3OPV3zrH1h6WGdsg9dw4\n",
            "3HSdCHlE54svpPr2w5zuxt\n",
            "6Iu1ycFXYHT8qDp5kxW41B\n",
            "6P1t6BzpTqTJeCBc8yhEi8\n",
            "4kvGRowUZYVL0mlRzxP3fC\n",
            "7mwAoSNGIUlSVz6aKae073\n",
            "2wjbX0KQjIxxZbfMmDmgQm\n",
            "65OEoYWUTkxMJa1ce9EdXO\n",
            "72f8rqrJWybX7n1ZWTgIj3\n",
            "5njlEFALN6ASvhJ0h3tD6g\n",
            "2eywYXsY2SpVHSAFyuKBM4\n",
            "6eKFuszqItq1xyWikZVmpI\n",
            "3jsYk453TbxgBMFmI7RHh0\n",
            "2yqmUtOLPJ68s2N3h3KXj7\n",
            "3sLDd0YdFpp43g9DIXWtp8\n",
            "1iTJNUElBgg614eNqkqaR0\n",
            "5dS3FAYxFSPeV51PelFr6s\n",
            "59L6rTh1kEd2lVR9VrtRJU\n",
            "25sW3wujQUMP2N8nL6Xb9r\n",
            "218cf54dIgUMc5ZJefuCAp\n",
            "5htNNx48E9AzGD5Tmu2UHU\n",
            "2JxOLvXns3zEQBJssbDpqs\n",
            "7hipXYaidVw3rTzqeh0Ic5\n",
            "7EhsY4RZ48NHi0Y1f08xTC\n",
            "0B41Kpmq4KeLBruZjZGqf1\n",
            "6gswKV6ZOVxesrb8mzCiUw\n",
            "1ZYtLraIw8Pg26r6ZaNFKl\n",
            "1huqALIj53pG41UvgnmsGP\n",
            "0vI6PY16sJBnB0TmTE8BCy\n",
            "2YUFRpBkxXP1EWygncnzGS\n",
            "7MsZgP7m2x2YxHvAUz014k\n",
            "6fqp1QkLOscqAphCb7Xhft\n",
            "0ltufIj5v6vTjzjoNkEcYg\n",
            "1SjFxUp7Iq5vF6oVXNDylr\n",
            "4sJi2vUtel5a7wfXozlwBc\n",
            "264qDOXvEfXRbhMHR3rFek\n",
            "2mDjiHE7tvC9Xs0FmshleH\n",
            "1zyyOU5kizmAzbI6IVhJlX\n",
            "4WeA0buDGLv5VaPyXp3qNl\n",
            "6y4OT3SdsoeV31S2swXeyg\n",
            "3FFE14n5zzWC8mExGsyuz2\n",
            "5HmJDriq2xJ1VzskNe0qt1\n",
            "0OAG6yYRHayZjdSHG2xFDo\n",
            "3pyy8dfg4i9gFb71tN2o8W\n",
            "24DSCKG6kT4cOaRY6X3Qwk\n",
            "5PMMl92vCQhrywbq7VidgX\n",
            "7j9Ry7yIVQ4csxt4XVZSlj\n",
            "4rKOjCqDONKDermjjkZY4p\n",
            "5VEkgjjZJuDvB97mwwGzuz\n",
            "2fDBgbdzVZ9aypFZ99iFFx\n",
            "3GvaEpOLKxfRwIBkCcATkU\n",
            "46E2o8mbxbtRjapxGrVs6S\n",
            "3JwJzd8oBciF0l4tXDrO3J\n",
            "72qZAlXOwpMEcxWbuJJSFN\n",
            "4YhEW2BqNgoIzKtXXQuUZN\n",
            "4KNQoxTDP0oJNY1rKn7hMy\n",
            "7xfPPfIGIMkEgE7n4xXn7W\n",
            "4aFDGJBR60BnJrAc0D0odE\n",
            "5ROPnQy85x5HWxni5Rw14m\n",
            "6dyjbBYVujaiITCAe1e2QZ\n",
            "7EMQtJFG6qmSfMP1i5RmPL\n",
            "5ZDA21x5NwXWPxiS8XyeAn\n",
            "6hdEJqaMb4A8XMZlzDs9dd\n",
            "09ETGvAQI21XzmPHE1ntXC\n",
            "6D2zNskJZeb2Wz640VnTrL\n",
            "36VAaIb50Ms2DFfJAwvCBy\n",
            "2MlANNCG8ByQl3yKo8YV33\n",
            "1ij6vBKQyQy9fYmNM80pfj\n",
            "6gNdCT8GJ5BapPjQmzHaro\n",
            "71wUN24dRMYipKz0OGkC6Q\n",
            "7ryn03CcZhC8K5uI9ku1sJ\n",
            "6X4NCOjvjBh65fNRlGhD4Q\n",
            "0sproPzjfPTGWtwTEcRObF\n",
            "7HSO793xOWZkl1DG5QrP6W\n",
            "2iExSJChra92HtmFS5UnNE\n",
            "7y92hNxDSVM0Gl04QHcuLk\n",
            "2EccM0GPevMpo6ZwebXTDB\n",
            "0IcFaJZ7rtaHB3ZFLuGo5k\n",
            "70WZLB59ojnYCdgENjGmeL\n",
            "7mdEJOL1QQkFLdFxCDlIRz\n",
            "4HUlI722uM9W7GmfgyvAvU\n",
            "7bf6LL0lCy8y0sp3cbhzE2\n",
            "4LEiyo6QNu367fdMou2no5\n",
            "1Tg35RC4cmnIuG2bZdXtm4\n",
            "7Eo1BNSQB2oCiNJMJuppQS\n",
            "1F4LvvGEgeeIlJoHaUlwXt\n",
            "2xJSU5kObzmSYu7sJBnwGy\n",
            "2cyksnSzVB04KKhzfhC7sr\n",
            "1drOsODEPNEc6YQkaILSjD\n",
            "2OJvTBvFpuhRmd0OWLArND\n",
            "17ycUtoidL65fFWeEoHuGU\n",
            "2IoZPi11gwnWxloGWTy4xK\n",
            "5IOW47xmAlq7BH9l8MG3Tx\n",
            "57bfpub7vydpv55kswhebt\n",
            "3a3yxbtj4bvl6s1zfbcz8c\n",
            "2h2fvekio7vft0csldfv25\n",
            "7gmz6lcnuk7uqqvzpa78qq\n",
            "1nihannz9p88wk4ggxyarg\n",
            "7etrmthygbwtojo6if5wgg\n",
            "2isxduowdojsccxj0bvdqw\n",
            "0kphugrsmwqcjdrz7bg905\n",
            "4bkieptxz6gyoquxqteeni\n",
            "3uishxrydhgvu0lvbievgo\n",
            "3brpa8zvgrtwnohklpqt1j\n",
            "6punxmbgxdknzbatlbapxi\n",
            "2mvv6rz1dzxvidypz4dayd\n",
            "3b0dlhiqhi7qljj4hyxck1\n",
            "1akrr4ht6ace4qanawq73s\n",
            "7xmfp8woqppf5ehxclzakx\n",
            "1o4q00ocfekscbbr6ozt5t\n",
            "5s1cgsmeqa95ljr2q7cq3p\n",
            "4epkso62sjrpt2xbryuguq\n",
            "71thihcyqbhzgkakypaylt\n",
            "1asn8yrhgqlnzgu3au5cql\n",
            "6sx29aytphxx6vhvage98d\n",
            "1euzbc1mjqw151dfjt61t2\n",
            "0y71qqe5xssci5h7ru9qim\n",
            "0ao0n77lk45pth1op6gh5p\n",
            "5feqyw5layck6fkgjtcffn\n",
            "4ccgqliaaxgenajjqxktmi\n",
            "09ti2kyt3vjtxdrs9p49eg\n",
            "4o30eet9hhgwkubxrws7uf\n",
            "5wu75et0fpqodawug7seij\n",
            "7ihhywephqgyeakfjbnvt6\n",
            "1xap5hzvvvp8tx0pmkzrv5\n",
            "5wecz9nyuewxcce3o0taqt\n",
            "3tuijhxmamowq5llbeine6\n",
            "7raavhpjdfpozgiay0rhub\n",
            "1xqmjtsow6ldbiwndaifg8\n",
            "1x2snoyu0m9uwbkj79gf0p\n",
            "1odonespzwczutbwe5cw0d\n",
            "0ujvorb4bdjfxzjf5qitxw\n",
            "5q0cqu1vk37bnsgyeefwte\n",
            "16meyav76scstbtfurgp60\n",
            "06hlmcn2v4isyfvr0vwxnk\n",
            "6vmgq9zek2evmhqmlil65p\n",
            "4bbylwclebl4nvpznv0ewo\n",
            "4mb0jot9gwt9u7ep9htqpa\n",
            "0b6higoabtdyk3gqmnpauf\n",
            "59gvqndl22jtydknoem2nu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "\n",
        "dir_path = '/content/gdrive/MyDrive/Spotify/subsets/Model_Output/Pegasus_Extractive_Abstractive_L2/'\n",
        " # Replace with your directory path\n",
        "\n",
        "file_count = len([f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))])\n",
        "\n",
        "print(f\"Number of files in {dir_path}: {file_count}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AepHlKmunqjr",
        "outputId": "790d359c-afd3-46c2-b214-17ab356eb1ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in /content/gdrive/MyDrive/Spotify/subsets/Model_Output/Pegasus_Extractive_Abstractive_L2/: 500\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}